{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d0a44-0aae-4a22-9ea2-552f1efd9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31791de-bdbf-44f4-ac17-de4f754d0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/sky_workdir/encoding-schemes\")\n",
    "\n",
    "from encoding_schemes import get_deterministic_adherence_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8432f-7bb7-4bb8-8223-ea329456a590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8ad9e-91d3-4e3b-b25c-9f19f3e4b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "conn_string = os.environ[\"SUPABASE_CONNECTION_URL\"]\n",
    "\n",
    "conn = psycopg2.connect(conn_string)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b3a2c-ade1-42ca-a798-5b94232485d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sel_str = \"\"\"\n",
    "-- NuminaMath CoT Rerun\n",
    " (\n",
    "     (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "     AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "     AND (\n",
    "         (data->'experiment_params'->'sampling_params'->'n')::INT = 4\n",
    "         OR ((data->'experiment_params'->'model')::TEXT LIKE '%gpt%' AND (data->'experiment_params'->'sft_params'->'batch_size')::INT != 48)\n",
    "     )\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(f\"\"\"\n",
    "SELECT * FROM public.encoding_schemes \n",
    "    WHERE \n",
    "\n",
    "{sel_str}\n",
    "\n",
    "\n",
    "ORDER BY created_at DESC\n",
    "\"\"\", conn)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439f59a-7755-482d-ac55-c324ac5bed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/ubuntu/sky_workdir/encoding-schemes/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d72f27-7ec8-4ffd-b530-f2b0b27cefab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_examples = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef482ac-2bcb-46ad-81d0-1a2e523a92e7",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a81ec-f561-430e-8570-c9d4a2a0f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_ci(data, statistic=np.mean, alpha=0.05, n_boot=10_000, random_state=None):\n",
    "    \"\"\"\n",
    "    Returns (point_estimate, low_CI, high_CI) for given 1D data.\n",
    "    Works with bool, int, or float data.\n",
    "    \"\"\"\n",
    "    x = np.asarray(data).astype(float)  # ensure numeric\n",
    "    x = x[~np.isnan(x)]\n",
    "    if len(x) == 0:\n",
    "        raise ValueError(\"No valid data for bootstrapping.\")\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = len(x)\n",
    "\n",
    "    # Draw bootstrap samples\n",
    "    idx = rng.integers(0, n, size=(n_boot, n))\n",
    "    samples = x[idx]\n",
    "\n",
    "    # Apply statistic row-wise\n",
    "    stats = np.apply_along_axis(statistic, 1, samples)\n",
    "\n",
    "    point = statistic(x)\n",
    "    lo = np.percentile(stats, 100 * (alpha / 2))\n",
    "    hi = np.percentile(stats, 100 * (1 - alpha / 2))\n",
    "    return point, lo, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fb9f9-33d1-4e61-b837-cc8b37219291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def count_tokens_from_messages(s):\n",
    "    try:\n",
    "        return len(encoding.encode(s, disallowed_special=()))\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db9fcc-ffa9-4a77-ad34-88067f4d63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=2, memory=32 * 1024 * 1024 * 1024)\n",
    "def compute_translation_token_count(example, df_data):\n",
    "    sys.path.append(\"/home/ubuntu/sky_workdir/encoding-schemes\")\n",
    "\n",
    "    from orchestration.experiment_meta_saver import compute_experiment_hash\n",
    "    from utils.io_utils import read_large_parquet\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = example[\"data\"][\"experiment_params\"][\"model\"]\n",
    "    if \"gpt\" in model or \"claude\" in model:\n",
    "        print(f\"Overriding tokenizer for {model} with gpt-oss 120b tokenizer because it was detected as a GPT/Claude model!\")\n",
    "        model = \"openai/gpt-oss-120b\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "    return df_data['reference_solution'].map(lambda x: len(tokenizer.encode(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f84e71-0f86-4e06-b744-96e3e6a71ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231976a-7a7d-4f2a-90f8-eaa70083c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ci_cols(example, df, col_name, transformation_fn):\n",
    "    s_transformed = transformation_fn(df[col_name])\n",
    "    if np.isscalar(s_transformed) and np.isnan(s_transformed):\n",
    "        print(f\"Warning: {col_name} was all NaN, ignoring!\")\n",
    "        return\n",
    "    mid, lo, hi = bootstrap_ci(s_transformed)\n",
    "    example[col_name] = mid\n",
    "    example[f'{col_name}_low_ci'] = mid - lo\n",
    "    example[f'{col_name}_hi_ci'] = hi - mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1d5fd-3931-4206-a956-9704eae789ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multi_row_transformation(df, row_transform_fn, col_name, agg_fn):\n",
    "    df[col_name] = df.apply(row_transform_fn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73907c00-ce16-4bce-b6e9-14d9bb68e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _score_rollouts(rollouts):\n",
    "    # rollouts: list/array of rollout sequences; may also be None/np.nan/scalar\n",
    "    if rollouts is None or (isinstance(rollouts, float) and np.isnan(rollouts)):\n",
    "        return np.nan\n",
    "\n",
    "    vals = []\n",
    "    for r in rollouts:\n",
    "        # skip None/NaN\n",
    "        if r is None or (isinstance(r, float) and np.isnan(r)):\n",
    "            continue\n",
    "        vals.append(-np.nansum(r))\n",
    "    return np.nanmean(vals) if len(vals) else np.nan\n",
    "\n",
    "\n",
    "def patch_gpt_api_log_loss(example):\n",
    "    experiment_hash = example['experiment_hash']\n",
    "\n",
    "    translation_loss = os.path.join(root_dir, experiment_hash, \"data\", f\"validation_reverse_translation_math500_meta.json\")\n",
    "    with open(translation_loss, \"r\") as fp:\n",
    "        example[\"backtranslation_gt_logprobs\"] = translation_loss[\"valid_loss\"]\n",
    "\n",
    "    # need to be validation loss on 512k...\n",
    "    validation_loss = os.path.join(root_dir, experiment_hash, \"data\", f\"validation_reverse_translation_math500_meta.json\")\n",
    "    with open(validation_loss, \"r\") as fp:\n",
    "        example[\"backtranslation_gt_logprobs\"] = translation_loss[\"valid_loss\"]\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def process_single_example(example):\n",
    "    experiment_hash = example['experiment_hash']\n",
    "    \n",
    "    target_path = os.path.join(root_dir, example['experiment_hash'], \"data\", \"joined_output.parquet\")\n",
    "    if not os.path.exists(target_path):\n",
    "        print(f\"!!!!!! {target_path} missing !!!!!!!\")\n",
    "        return example\n",
    "    \n",
    "    df_data = pd.read_parquet(target_path)\n",
    "\n",
    "    d_col_to_transform = {\n",
    "        'cot_gt_logprobs' : lambda s: np.nansum(s.map(_score_rollouts)),\n",
    "        'generated_cot_is_correct' : np.mean,  # was np.mean\n",
    "        'backtranslation_gt_logprobs' : lambda s: np.nanmean(s.map(_score_rollouts)),\n",
    "        'backtranslation_bleu_scores' : np.mean,  # was np.mean\n",
    "        'generated_cot_adhered_encoding_style': np.mean  # was np.mean\n",
    "    }\n",
    "    for col, fn in d_col_to_transform.items():\n",
    "        if col not in df_data:\n",
    "            print(col)\n",
    "            print(df_data.head())\n",
    "            print(example)\n",
    "            raise Exception(str(col) + \"\\n\" + str(df_data.head()) + \"\\n\" + str(example))\n",
    "\n",
    "        compute_ci_cols(example, df_data, col, fn)\n",
    "\n",
    "    df_data[\"num_tokens_translation_output\"] = ray.get(compute_translation_token_count.remote(example, df_data))\n",
    "\n",
    "    d_and_cols = {\n",
    "        'adherent_and_correct': (\n",
    "            lambda r: np.nanmean( \\\n",
    "                np.array(r['generated_cot_is_correct']).astype(bool) & \\\n",
    "                np.array(r['generated_cot_adhered_encoding_style']).astype(bool) \\\n",
    "            ),\n",
    "            np.nanmean\n",
    "        ),\n",
    "        'total_translation_loss': (\n",
    "            lambda r: np.nanmean( \\\n",
    "                np.array(r['num_tokens_translation_output']) * \\\n",
    "                np.array(np.nanmean(_score_rollouts(r['backtranslation_gt_logprobs'])), dtype=np.float64) \\\n",
    "            ),\n",
    "            np.nanmean\n",
    "        ),\n",
    "    }\n",
    "    for col, (transform_fn, agg_fn) in d_and_cols.items():\n",
    "        compute_multi_row_transformation(df_data, transform_fn, col, agg_fn)\n",
    "        if df_data[col].isna().sum() != len(df_data):\n",
    "            compute_ci_cols(example, df_data, col, lambda x: x)\n",
    "\n",
    "    if \"gpt\" in example[\"data\"][\"experiment_params\"][\"model\"] and \"use_api_sft_model_for_sampling\" in example[\"data\"][\"experiment_params\"]:\n",
    "        with open(os.path.join(root_dir, example['experiment_hash'], \"data\", f\"validation_reverse_translation_math500_meta.json\"), \"r\") as fp:\n",
    "            d_model_meta = json.load(fp)\n",
    "\n",
    "        example[\"backtranslation_gt_logprobs\"] = d_model_meta[\"valid_loss\"]\n",
    "        example[\"total_translation_loss\"] = d_model_meta[\"valid_loss\"] * np.nanmean(df_data[\"num_tokens_translation_output\"])\n",
    "        example[\"total_translation_loss_low_ci\"] = 0.0\n",
    "        example[\"total_translation_loss_hi_ci\"] = 0.0\n",
    "\n",
    "    pretraining_prevalence_path = os.path.join('/home/ubuntu/sky_workdir/encoding-schemes', 'output', experiment_hash, 'data', 'num_pretraining_4grams_redpajama.json')\n",
    "    if os.path.exists(pretraining_prevalence_path):\n",
    "        with open(pretraining_prevalence_path, \"r\") as fp:\n",
    "            d_pretraining_prevalence = json.load(fp)\n",
    "\n",
    "        example[\"pretraining_prevalence\"] = d_pretraining_prevalence[\"num_occurrences\"]\n",
    "\n",
    "    for col in df_data.columns:\n",
    "        example[f\"{col}_df\"] = df_data[col]\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "l_new_examples = [None for _ in range(len(l_examples))]\n",
    "\n",
    "for i, example in tqdm(enumerate(l_examples)):\n",
    "    # l_examples[i] = process_single_example(example)\n",
    "    l_new_examples[i] = process_single_example.remote(example)\n",
    "\n",
    "for i, example in tqdm(enumerate(l_new_examples)):\n",
    "    try:\n",
    "        l_new_examples[i] = ray.get(example)\n",
    "    except ray.exceptions.RayTaskError as e:\n",
    "        l_new_examples[i] = l_examples[i]\n",
    "        print(e)\n",
    "\n",
    "l_examples = l_new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7b0cc-4d72-4780-9f3a-0fa25ee3a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanize_number(num: float) -> str:\n",
    "    \"\"\"\n",
    "    Converts a number into a human-readable string with k, M, or B suffixes.\n",
    "    \n",
    "    Args:\n",
    "        num (float): The number to format.\n",
    "    \n",
    "    Returns:\n",
    "        str: Human-readable string representation.\n",
    "    \"\"\"\n",
    "    if num >= 1_000_000_000:\n",
    "        return f\"{num / 1_000_000_000:.1f}B\"\n",
    "    elif num >= 1_000_000:\n",
    "        return f\"{num / 1_000_000:.1f}M\"\n",
    "    elif num >= 1_000:\n",
    "        return f\"{num / 1_000:.1f}k\"\n",
    "    else:\n",
    "        return str(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c8aa3-b7b1-48a7-b0fc-c8811fe9a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_params(model):\n",
    "    if 'gpt' in model:\n",
    "        if 'nano' in model:\n",
    "            return 0\n",
    "        elif 'mini' in model:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "\n",
    "    if 'claude' in model:\n",
    "        if 'haiku' in model:\n",
    "            return 3\n",
    "        elif 'sonnet' in model:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "    \n",
    "    return int(re.search(\"([0-9]+)B\", model).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a412a1f-de17-4d22-b798-12ea8ce0db67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_viz = pd.DataFrame(l_examples)\n",
    "\n",
    "orig_len = len(df_viz)\n",
    "\n",
    "# df_viz = df_viz[df_viz['cot_gt_logprobs'].notna()]\n",
    "\n",
    "new_len = len(df_viz)\n",
    "if orig_len != new_len:\n",
    "    print(f\"Lost {orig_len - new_len} examples from na logprobs\")\n",
    "\n",
    "df_viz['encoding_scheme'] = df_viz['data'].map(lambda x: x['experiment_params']['encoding_scheme'])\n",
    "df_viz['model'] = df_viz['data'].map(lambda x: x['experiment_params']['model'])\n",
    "\n",
    "try:\n",
    "    df_viz['model_size'] = df_viz['model'].map(parse_params)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "df_viz['input_type'] = df_viz['data'].map(lambda x: \"_\".join(x['experiment_name'].split(\"_\")[:2]))\n",
    "\n",
    "df_viz['n_few_shot_examples'] = df_viz['data'].map(lambda x: x['experiment_params'].get('n_few_shot_examples', None))\n",
    "\n",
    "df_viz['Adherence Calculation Method'] = df_viz['encoding_scheme'].map(lambda x: get_deterministic_adherence_fn(x, None) is not None).map({ True: 'deterministic', False: 'Sonnet 4 judge'})\n",
    "\n",
    "try:\n",
    "    df_viz['total_train_tok'] = df_viz['n_total_train_tok'].map(humanize_number)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59da245-a8a4-482d-b5a8-814c663f4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz['input_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d5f60-3ce3-448f-b887-0d0723e8a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_set = ['math_cot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294af9ce-482f-4336-a8bc-6aaf90027383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp = df_viz[df_viz['input_type'].isin(filter_set)]\n",
    "df_viz_tmp = df_viz_tmp[df_viz_tmp['model'] != 'Qwen/Qwen2.5-7B']\n",
    "\n",
    "df_viz_tmp = df_viz_tmp.sort_values([\n",
    "    'model_size',\n",
    "    'adherent_and_correct'\n",
    "])\n",
    "\n",
    "df_viz_tmp = df_viz_tmp.astype({'n_few_shot_examples': str})\n",
    "\n",
    "df_viz_tmp['encoding_scheme'] = df_viz_tmp['encoding_scheme'].map(lambda s: s.split(\"speaking_\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559e93b-ed7e-473a-b1e1-e40b2d723072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "\n",
    "fig = px.bar(df_viz_tmp_plot, x='encoding_scheme', y='generated_cot_adhered_encoding_style',\n",
    "             height=800, width=1600,\n",
    "             # height=1600, width=1600,\n",
    "             # color='model',\n",
    "             error_y='generated_cot_adhered_encoding_style_hi_ci',\n",
    "             error_y_minus='generated_cot_adhered_encoding_style_low_ci',\n",
    "             # color='n_few_shot_examples',\n",
    "             # facet_row='model',\n",
    "             color='model',\n",
    "             facet_col='Adherence Calculation Method',\n",
    "             # color='training_augmentation',\n",
    "             # color='total_train_tok',\n",
    "             # color_discrete_map=color_discrete_map,\n",
    "             title=\"MATH 500 CoT encoding style adherence\",\n",
    "             barmode=\"group\",\n",
    "             template=\"plotly_white\",\n",
    "             color_discrete_sequence=px.colors.qualitative.Set2,\n",
    "            )\n",
    "\n",
    "fig.update_xaxes(title=\"Encoding scheme\", tickangle=90)\n",
    "fig.update_yaxes(title=\"% adherent encodings\", dtick=0.1)\n",
    "\n",
    "# ✅ Make each facet's x-axis independent to avoid empty slots\n",
    "fig.for_each_xaxis(lambda ax: ax.update(matches=None, categoryorder='trace'))\n",
    "\n",
    "# ✅ Align all x-axis titles by setting a fixed vertical offset\n",
    "ct = [0]\n",
    "def ax_standoff_updater(ax, ct):    \n",
    "    ax.title.update(standoff=ct[0] * 130)\n",
    "    ct[0] += 1\n",
    "\n",
    "fig.for_each_xaxis(lambda ax: ax_standoff_updater(ax, ct))\n",
    "fig.update_yaxes(title_standoff=5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c9963-fa36-472f-ae78-e4eba8dacc45",
   "metadata": {},
   "source": [
    "# Pretraining prevalence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636c594-1907-4a0d-bd0a-62aef13b926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "df_viz_tmp_plot['encoding_scheme'] = df_viz_tmp_plot['encoding_scheme'].str.replace('_', ' ')\n",
    "\n",
    "l_algo_encodings = [\n",
    "    'base64 cipher',\n",
    "    'base64 2x cipher',\n",
    "    'base64 3x cipher',\n",
    "    'gzip to base64 encoded',\n",
    "    'letter to word with dot',\n",
    "    'dot between chars',\n",
    "    'space between chars'\n",
    "]\n",
    "\n",
    "# Separate data into two groups\n",
    "df_l_algo = df_viz_tmp_plot[df_viz_tmp_plot['encoding_scheme'].isin(l_algo_encodings)]\n",
    "df_l_algo = pd.concat([\n",
    "    df_l_algo,\n",
    "    df_viz_tmp_plot[df_viz_tmp_plot['encoding_scheme'] == 'identity']\n",
    "], ignore_index=True)\n",
    "\n",
    "df_rest = df_viz_tmp_plot[~df_viz_tmp_plot['encoding_scheme'].isin(l_algo_encodings)]\n",
    "\n",
    "# Create the scatter plot with color coding\n",
    "df_viz_tmp_plot['group'] = df_viz_tmp_plot['encoding_scheme'].apply(\n",
    "    lambda x: 'Algorithmic' if x in l_algo_encodings else 'identity' if x == 'identity' else 'Language'\n",
    ")\n",
    "\n",
    "fig = px.scatter(df_viz_tmp_plot, x='pretraining_prevalence', y='generated_cot_is_correct', \n",
    "                 log_x=True, text='encoding_scheme', color='group',\n",
    "                 color_discrete_map={'Algorithmic': 'red', 'Language': 'blue', 'identity': 'magenta'})\n",
    "\n",
    "# Update the scatter trace to not be part of the legend and keep its original appearance\n",
    "fig.update_traces(textposition=\"top center\", showlegend=False, selector=dict(mode='markers+text'))\n",
    "fig.update_layout(legend_title_text=None) \n",
    "\n",
    "# Define fit functions\n",
    "def linear_func(x, a, b):\n",
    "    \"\"\"Linear function: y = ax + b\"\"\"\n",
    "    return a * x + b\n",
    "\n",
    "def exponential_func(x, a, b, c):\n",
    "    \"\"\"Exponential function: y = a * exp(b * x) + c\"\"\"\n",
    "    return a * np.exp(b * x) + c\n",
    "\n",
    "def sigmoid_func(x, a, b, c, d):\n",
    "    \"\"\"Sigmoid function: y = a / (1 + exp(-b * (x - c))) + d\"\"\"\n",
    "    return a / (1 + np.exp(-b * (x - c))) + d\n",
    "\n",
    "def power_law_func(x, a, b):\n",
    "    \"\"\"Power law function: y = a * x^b\"\"\"\n",
    "    return a * np.power(x, b)\n",
    "\n",
    "def calculate_fit(df, fit_type='linear', use_log_x=True):\n",
    "    \"\"\"\n",
    "    Calculate trend line based on specified fit type.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with data\n",
    "    - fit_type: 'linear', 'exponential', 'sigmoid', or 'power_law'\n",
    "    - use_log_x: Whether to use log-transformed x values\n",
    "    \"\"\"\n",
    "    # Remove any rows with NaN or zero/negative x values\n",
    "    df_clean = df.dropna(subset=['pretraining_prevalence', 'generated_cot_is_correct'])\n",
    "    df_clean = df_clean[df_clean['pretraining_prevalence'] > 0]\n",
    "    \n",
    "    if len(df_clean) < 2:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Prepare x and y values\n",
    "    x_original = df_clean['pretraining_prevalence'].values\n",
    "    y = df_clean['generated_cot_is_correct'].values\n",
    "    \n",
    "    # For power law, we don't want to log-transform x if we're fitting directly\n",
    "    if fit_type == 'power_law':\n",
    "        x = x_original\n",
    "    elif use_log_x:\n",
    "        x = np.log10(x_original)\n",
    "    else:\n",
    "        x = x_original\n",
    "    \n",
    "    try:\n",
    "        if fit_type == 'linear':\n",
    "            # Linear regression\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            params = [slope, intercept]\n",
    "            \n",
    "            # Calculate predictions for R²\n",
    "            y_pred = linear_func(x, *params)\n",
    "            \n",
    "        elif fit_type == 'exponential':\n",
    "            # Exponential fit\n",
    "            # Initial guess: [amplitude, growth_rate, offset]\n",
    "            initial_guess = [0.5, 0.1, 0.3]\n",
    "            bounds = ([0, -np.inf, 0], [1, np.inf, 1])\n",
    "            params, _ = curve_fit(exponential_func, x, y, p0=initial_guess, \n",
    "                                bounds=bounds, maxfev=5000)\n",
    "            \n",
    "            # Calculate predictions for R²\n",
    "            y_pred = exponential_func(x, *params)\n",
    "            \n",
    "        elif fit_type == 'logistic':\n",
    "            # Sigmoid fit\n",
    "            # Initial guess: [amplitude, steepness, x_mid, y_offset]\n",
    "            x_mid = np.median(x)\n",
    "            initial_guess = [0.7, 1.0, x_mid, 0.2]\n",
    "            bounds = ([0, 0, x.min(), 0], [1, 10, x.max(), 1])\n",
    "            params, _ = curve_fit(sigmoid_func, x, y, p0=initial_guess, \n",
    "                                bounds=bounds, maxfev=5000)\n",
    "            \n",
    "            # Calculate predictions for R²\n",
    "            y_pred = sigmoid_func(x, *params)\n",
    "            \n",
    "        elif fit_type == 'power_law':\n",
    "            # Power law fit\n",
    "            # For numerical stability, we can fit log(y) = log(a) + b*log(x)\n",
    "            # Then convert back: a = exp(log_a), b = b\n",
    "            \n",
    "            # Remove any y values that are <= 0 for log transformation\n",
    "            mask = y > 0\n",
    "            if np.sum(mask) < 2:\n",
    "                return None, None, None\n",
    "            \n",
    "            x_fit = x[mask]\n",
    "            y_fit = y[mask]\n",
    "            \n",
    "            # Two approaches - try both and use the one with better R²\n",
    "            \n",
    "            # Approach 1: Direct fit\n",
    "            try:\n",
    "                initial_guess = [1e-10, 0.5]\n",
    "                bounds = ([1e-15, -10], [1, 10])\n",
    "                params1, _ = curve_fit(power_law_func, x_fit, y_fit, \n",
    "                                      p0=initial_guess, bounds=bounds, maxfev=5000)\n",
    "                y_pred1 = power_law_func(x, *params1)\n",
    "                ss_res1 = np.sum((y - y_pred1) ** 2)\n",
    "                ss_tot1 = np.sum((y - np.mean(y)) ** 2)\n",
    "                r2_1 = 1 - (ss_res1 / ss_tot1) if ss_tot1 > 0 else -np.inf\n",
    "            except:\n",
    "                r2_1 = -np.inf\n",
    "                params1 = None\n",
    "            \n",
    "            # Approach 2: Log-log linear regression\n",
    "            try:\n",
    "                log_x = np.log10(x_fit)\n",
    "                log_y = np.log10(y_fit)\n",
    "                slope, log_a, r_value, p_value, std_err = stats.linregress(log_x, log_y)\n",
    "                a = 10**log_a\n",
    "                b = slope\n",
    "                params2 = [a, b]\n",
    "                y_pred2 = power_law_func(x, *params2)\n",
    "                ss_res2 = np.sum((y - y_pred2) ** 2)\n",
    "                ss_tot2 = np.sum((y - np.mean(y)) ** 2)\n",
    "                r2_2 = 1 - (ss_res2 / ss_tot2) if ss_tot2 > 0 else -np.inf\n",
    "            except:\n",
    "                r2_2 = -np.inf\n",
    "                params2 = None\n",
    "            \n",
    "            # Choose the better fit\n",
    "            if r2_1 > r2_2 and params1 is not None:\n",
    "                params = params1\n",
    "                y_pred = y_pred1\n",
    "            elif params2 is not None:\n",
    "                params = params2\n",
    "                y_pred = y_pred2\n",
    "            else:\n",
    "                return None, None, None\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fit_type: {fit_type}\")\n",
    "        \n",
    "        # Calculate R²\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "        \n",
    "        return params, r_squared, fit_type\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting {fit_type}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Configuration - CHANGE THESE TO SELECT FIT TYPE\n",
    "FIT_TYPE = 'logistic'  # Options: 'linear', 'exponential', 'sigmoid', 'power_law'\n",
    "USE_LOG_X = True  # Whether to use log-transformed x for fitting (not used for power_law)\n",
    "\n",
    "# Calculate trends for both groups\n",
    "params_l_algo, r2_l_algo, _ = calculate_fit(df_l_algo, fit_type=FIT_TYPE, use_log_x=USE_LOG_X)\n",
    "params_rest, r2_rest, _ = calculate_fit(df_rest, fit_type=FIT_TYPE, use_log_x=USE_LOG_X)\n",
    "\n",
    "# Generate trend line points\n",
    "x_range = df_viz_tmp_plot['pretraining_prevalence'].values\n",
    "x_range = x_range[x_range > 0]  # Remove non-positive values\n",
    "x_range = np.sort(x_range)\n",
    "\n",
    "if FIT_TYPE == 'power_law':\n",
    "    x_fit_range = x_range\n",
    "elif USE_LOG_X:\n",
    "    x_fit_range = np.log10(x_range)\n",
    "else:\n",
    "    x_fit_range = x_range\n",
    "\n",
    "# Add trend lines if calculations were successful\n",
    "if params_l_algo is not None:\n",
    "    if FIT_TYPE == 'linear':\n",
    "        y_trend_l_algo = linear_func(x_fit_range, *params_l_algo)\n",
    "    elif FIT_TYPE == 'exponential':\n",
    "        y_trend_l_algo = exponential_func(x_fit_range, *params_l_algo)\n",
    "    elif FIT_TYPE == 'logistic':\n",
    "        y_trend_l_algo = sigmoid_func(x_fit_range, *params_l_algo)\n",
    "    elif FIT_TYPE == 'power_law':\n",
    "        y_trend_l_algo = power_law_func(x_fit_range, *params_l_algo)\n",
    "    \n",
    "    # Format the equation for power law\n",
    "    if FIT_TYPE == 'power_law':\n",
    "        eq_str = f'y = {params_l_algo[0]:.2e} * x^{params_l_algo[1]:.3f}'\n",
    "        name_str = f'Algorithmic encoding, power law (R²={r2_l_algo:.3f})'\n",
    "    else:\n",
    "        name_str = f'Structure-disrupting cipher, {FIT_TYPE} fit (R²={r2_l_algo:.3f})'\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_range[:-9],\n",
    "        y=y_trend_l_algo[:-9],\n",
    "        mode='lines',\n",
    "        name=name_str,\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "if params_rest is not None:\n",
    "    if FIT_TYPE == 'linear':\n",
    "        y_trend_rest = linear_func(x_fit_range, *params_rest)\n",
    "    elif FIT_TYPE == 'exponential':\n",
    "        y_trend_rest = exponential_func(x_fit_range, *params_rest)\n",
    "    elif FIT_TYPE == 'logistic':\n",
    "        y_trend_rest = sigmoid_func(x_fit_range, *params_rest)\n",
    "    elif FIT_TYPE == 'power_law':\n",
    "        y_trend_rest = power_law_func(x_fit_range, *params_rest)\n",
    "    \n",
    "    # Format the equation for power law\n",
    "    if FIT_TYPE == 'power_law':\n",
    "        eq_str = f'y = {params_rest[0]:.2e} * x^{params_rest[1]:.3f}'\n",
    "        name_str = f'Language encoding, power law (R²={r2_rest:.3f})'\n",
    "    else:\n",
    "        name_str = f'Structure-preserving cipher, {FIT_TYPE} fit (R²={r2_rest:.3f})'\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_range[6:],\n",
    "        y=y_trend_rest[6:],\n",
    "        mode='lines',\n",
    "        name=name_str,\n",
    "        line=dict(color='blue', dash='dash'),\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "# Print the power law parameters if that's what we're using\n",
    "if FIT_TYPE == 'power_law':\n",
    "    if params_l_algo is not None:\n",
    "        print(f\"Algorithmic encoding power law: y = {params_l_algo[0]:.2e} * x^{params_l_algo[1]:.3f}\")\n",
    "    if params_rest is not None:\n",
    "        print(f\"Language encoding power law: y = {params_rest[0]:.2e} * x^{params_rest[1]:.3f}\")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    width=800,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ),\n",
    "    # title=f\"MATH500 Accuracy vs Pretraining Prevalence ({FIT_TYPE.capitalize().replace('_', ' ')} Fit)\"\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "fig.update_yaxes(dtick=0.1, title=\"MATH500 accuracy\", range=[0, 0.8])\n",
    "fig.update_xaxes(title=\"Pretraining prevalence\", type=\"log\", range=[8.8, 10.5])\n",
    "fig.update_layout(template=\"plotly_white\", font=dict(size=22))\n",
    "fig.update_traces(textfont_size=16)\n",
    "fig.update_traces(marker=dict(size=9))\n",
    "\n",
    "def format_tokens(value):\n",
    "    if value >= 1_000_000_000:\n",
    "        return f'{value/1_000_000_000:.0f}B' if value % 1_000_000_000 == 0 else f'{value/1_000_000_000:.1f}B'\n",
    "    elif value >= 1_000_000:\n",
    "        return f'{value/1_000_000:.0f}M' if value % 1_000_000 == 0 else f'{value/1_000_000:.1f}M'\n",
    "    else:\n",
    "        return str(value)\n",
    "\n",
    "# Generate tickvals based on your data range\n",
    "tickvals = [700_000_000, 1_000_000_000, 2_000_000_000, 3_000_000_000, 4_000_000_000, 5_000_000_000, 7_000_000_000, 10_000_000_000, 20_000_000_000, 30_000_000_000]\n",
    "ticktext = [format_tokens(val) for val in tickvals]\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title=\"Pretraining prevalence\",\n",
    "    tickvals=tickvals,\n",
    "    ticktext=ticktext,\n",
    "    tickmode='array'\n",
    ")\n",
    "\n",
    "\n",
    "plotly.io.write_image(fig, 'pretraining_scaling_perf.pdf', format='pdf')\n",
    "fig.show('png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf110df-ad4e-4fe5-8ba5-340f5b034c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75965ca5-8c02-4a6e-b2f7-b9a510870a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33d894-e729-4149-95d6-2e3eeb8c0dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
