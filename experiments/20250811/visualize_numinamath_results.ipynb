{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d0a44-0aae-4a22-9ea2-552f1efd9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import duckdb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31791de-bdbf-44f4-ac17-de4f754d0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/sky_workdir/encoding-schemes\")\n",
    "\n",
    "from encoding_schemes import get_deterministic_adherence_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8432f-7bb7-4bb8-8223-ea329456a590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8ad9e-91d3-4e3b-b25c-9f19f3e4b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "conn_string = os.environ[\"SUPABASE_CONNECTION_URL\"]\n",
    "\n",
    "conn = psycopg2.connect(conn_string)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b3a2c-ade1-42ca-a798-5b94232485d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_sql(\"SELECT * FROM public.encoding_schemes WHERE (data->'experiment_tags'->'sft')::boolean\", conn)\n",
    "\n",
    "# sel_str = \"\"\"\n",
    "# -- redo prompted\n",
    "# (\n",
    "#     (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "#     AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "#     AND (data->'experiment_name')::TEXT LIKE '%prompted_%'\n",
    "# )\n",
    "# \"\"\"\n",
    "\n",
    "# sel_str = \"\"\"\n",
    "# -- Few shot\n",
    "#  (\n",
    "#      (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "#      AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "#      AND (\n",
    "#          (data->'experiment_params'->'n_few_shot_examples')::INT = 8\n",
    "#      )\n",
    "#   )\n",
    "# \"\"\"\n",
    "\n",
    "# sel_str = \"\"\"\n",
    "# -- NuminaMath CoT Rerun\n",
    "#  (\n",
    "#      (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "#      AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "#      AND (\n",
    "#          (data->'experiment_params'->'sampling_params'->'n')::INT = 4\n",
    "#          OR (data->'experiment_params'->'model')::TEXT LIKE '%gpt%'\n",
    "#      )\n",
    "#   )\n",
    "# \"\"\"\n",
    "\n",
    "sel_str = \"\"\"\n",
    "-- prompted no sft decode\n",
    "(\n",
    "    (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "    AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "    AND (data->'experiment_name')::TEXT LIKE '%prompteddecode%'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(f\"\"\"\n",
    "SELECT * FROM public.encoding_schemes \n",
    "    WHERE \n",
    "\n",
    "{sel_str}\n",
    "\n",
    "\n",
    "ORDER BY created_at DESC\n",
    "\"\"\", conn)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439f59a-7755-482d-ac55-c324ac5bed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/ubuntu/sky_workdir/encoding-schemes/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d72f27-7ec8-4ffd-b530-f2b0b27cefab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_examples = df.to_dict('records')\n",
    "\n",
    "l_examples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0754888-dc53-4d5c-a6e4-5c371360a198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[example for example in l_examples if example[\"data\"][\"experiment_params\"]['encoding_scheme'] == 'speaking_reverse_letters_in_each_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af06df-0d84-4284-ba43-74d9e9e7e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/c9abad32f9ae149eab5412db77c84c1992f350ba/data/joined_output.parquet\")\n",
    "\n",
    "df_sft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a81ec-f561-430e-8570-c9d4a2a0f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_ci(data, statistic=np.mean, alpha=0.05, n_boot=10_000, random_state=None):\n",
    "    \"\"\"\n",
    "    Returns (point_estimate, low_CI, high_CI) for given 1D data.\n",
    "    Works with bool, int, or float data.\n",
    "    \"\"\"\n",
    "    x = np.asarray(data).astype(float)  # ensure numeric\n",
    "    x = x[~np.isnan(x)]\n",
    "    if len(x) == 0:\n",
    "        raise ValueError(\"No valid data for bootstrapping.\")\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = len(x)\n",
    "\n",
    "    # Draw bootstrap samples\n",
    "    idx = rng.integers(0, n, size=(n_boot, n))\n",
    "    samples = x[idx]\n",
    "\n",
    "    # Apply statistic row-wise\n",
    "    stats = np.apply_along_axis(statistic, 1, samples)\n",
    "\n",
    "    point = statistic(x)\n",
    "    lo = np.percentile(stats, 100 * (alpha / 2))\n",
    "    hi = np.percentile(stats, 100 * (1 - alpha / 2))\n",
    "    return point, lo, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fb9f9-33d1-4e61-b837-cc8b37219291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def count_tokens_from_messages(s):\n",
    "    try:\n",
    "        return len(encoding.encode(s, disallowed_special=()))\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db9fcc-ffa9-4a77-ad34-88067f4d63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=2, memory=32 * 1024 * 1024 * 1024)\n",
    "def compute_translation_token_count(example, df_data):\n",
    "    sys.path.append(\"/home/ubuntu/sky_workdir/encoding-schemes\")\n",
    "\n",
    "    from orchestration.experiment_meta_saver import compute_experiment_hash\n",
    "    from utils.io_utils import read_large_parquet\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = example[\"data\"][\"experiment_params\"][\"model\"]\n",
    "    if \"gpt\" in model or \"claude\" in model:\n",
    "        print(f\"Overriding tokenizer for {model} with gpt-oss 120b tokenizer because it was detected as a GPT/Claude model!\")\n",
    "        model = \"openai/gpt-oss-120b\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "    return df_data['reference_solution'].map(lambda x: len(tokenizer.encode(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f84e71-0f86-4e06-b744-96e3e6a71ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231976a-7a7d-4f2a-90f8-eaa70083c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ci_cols(example, df, col_name, transformation_fn):\n",
    "    s_transformed = transformation_fn(df[col_name])\n",
    "    if np.isscalar(s_transformed) and np.isnan(s_transformed):\n",
    "        print(f\"Warning: {col_name} was all NaN, ignoring!\")\n",
    "        return\n",
    "    mid, lo, hi = bootstrap_ci(s_transformed)\n",
    "    example[col_name] = mid\n",
    "    example[f'{col_name}_low_ci'] = mid - lo\n",
    "    example[f'{col_name}_hi_ci'] = hi - mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1d5fd-3931-4206-a956-9704eae789ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multi_row_transformation(df, row_transform_fn, col_name, agg_fn):\n",
    "    df[col_name] = df.apply(row_transform_fn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73907c00-ce16-4bce-b6e9-14d9bb68e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _score_rollouts(rollouts):\n",
    "    # rollouts: list/array of rollout sequences; may also be None/np.nan/scalar\n",
    "    if rollouts is None or (isinstance(rollouts, float) and np.isnan(rollouts)):\n",
    "        return np.nan\n",
    "\n",
    "    vals = []\n",
    "    for r in rollouts:\n",
    "        # skip None/NaN\n",
    "        if r is None or (isinstance(r, float) and np.isnan(r)):\n",
    "            continue\n",
    "        vals.append(-np.nansum(r))\n",
    "    return np.nanmean(vals) if len(vals) else np.nan\n",
    "\n",
    "\n",
    "def patch_gpt_api_log_loss(example):\n",
    "    experiment_hash = example['experiment_hash']\n",
    "\n",
    "    translation_loss = os.path.join(root_dir, experiment_hash, \"data\", f\"validation_reverse_translation_math500_meta.json\")\n",
    "    with open(translation_loss, \"r\") as fp:\n",
    "        example[\"backtranslation_gt_logprobs\"] = translation_loss[\"valid_loss\"]\n",
    "\n",
    "    # need to be validation loss on 512k...\n",
    "    validation_loss = os.path.join(root_dir, experiment_hash, \"data\", f\"validation_reverse_translation_math500_meta.json\")\n",
    "    with open(validation_loss, \"r\") as fp:\n",
    "        example[\"backtranslation_gt_logprobs\"] = translation_loss[\"valid_loss\"]\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def process_single_example(example):\n",
    "    target_path = os.path.join(root_dir, example['experiment_hash'], \"data\", \"joined_output.parquet\")\n",
    "    if not os.path.exists(target_path):\n",
    "        print(f\"!!!!!! {target_path} missing !!!!!!!\")\n",
    "        return example\n",
    "    \n",
    "    df_data = pd.read_parquet(target_path)\n",
    "\n",
    "    d_col_to_transform = {\n",
    "        'cot_gt_logprobs' : lambda s: np.nansum(s.map(_score_rollouts)),\n",
    "        'generated_cot_is_correct' : np.mean,  # was np.mean\n",
    "        'backtranslation_gt_logprobs' : lambda s: np.nanmean(s.map(_score_rollouts)),\n",
    "        'backtranslation_bleu_scores' : np.mean,  # was np.mean\n",
    "        'generated_cot_adhered_encoding_style': np.mean  # was np.mean\n",
    "    }\n",
    "    for col, fn in d_col_to_transform.items():\n",
    "        if col not in df_data:\n",
    "            print(col)\n",
    "            print(df_data.head())\n",
    "            print(example)\n",
    "            raise Exception(str(col) + \"\\n\" + str(df_data.head()) + \"\\n\" + str(example))\n",
    "\n",
    "        compute_ci_cols(example, df_data, col, fn)\n",
    "\n",
    "    df_data[\"num_tokens_translation_output\"] = ray.get(compute_translation_token_count.remote(example, df_data))\n",
    "\n",
    "    d_and_cols = {\n",
    "        'adherent_and_correct': (\n",
    "            lambda r: np.nanmean( \\\n",
    "                np.array(r['generated_cot_is_correct']).astype(bool) & \\\n",
    "                np.array(r['generated_cot_adhered_encoding_style']).astype(bool) \\\n",
    "            ),\n",
    "            np.nanmean\n",
    "        ),\n",
    "        'total_translation_loss': (\n",
    "            lambda r: np.nanmean( \\\n",
    "                np.array(r['num_tokens_translation_output']) * \\\n",
    "                np.array(np.nanmean(_score_rollouts(r['backtranslation_gt_logprobs'])), dtype=np.float64) \\\n",
    "            ),\n",
    "            np.nanmean\n",
    "        ),\n",
    "    }\n",
    "    for col, (transform_fn, agg_fn) in d_and_cols.items():\n",
    "        compute_multi_row_transformation(df_data, transform_fn, col, agg_fn)\n",
    "        if df_data[col].isna().sum() != len(df_data):\n",
    "            compute_ci_cols(example, df_data, col, lambda x: x)\n",
    "\n",
    "    if \"gpt\" in example[\"data\"][\"experiment_params\"][\"model\"]:\n",
    "        with open(os.path.join(root_dir, example['experiment_hash'], \"data\", f\"validation_reverse_translation_math500_meta.json\"), \"r\") as fp:\n",
    "            d_model_meta = json.load(fp)\n",
    "\n",
    "        example[\"backtranslation_gt_logprobs\"] = d_model_meta[\"valid_loss\"]\n",
    "        example[\"total_translation_loss\"] = d_model_meta[\"valid_loss\"] * np.nanmean(df_data[\"num_tokens_translation_output\"])\n",
    "\n",
    "    for col in df_data.columns:\n",
    "        example[f\"{col}_df\"] = df_data[col]\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "l_new_examples = [None for _ in range(len(l_examples))]\n",
    "\n",
    "for i, example in tqdm(enumerate(l_examples)):\n",
    "    # l_examples[i] = process_single_example(example)\n",
    "    l_new_examples[i] = process_single_example.remote(example)\n",
    "\n",
    "for i, example in tqdm(enumerate(l_new_examples)):\n",
    "    try:\n",
    "        l_new_examples[i] = ray.get(example)\n",
    "    except ray.exceptions.RayTaskError as e:\n",
    "        l_new_examples[i] = l_examples[i]\n",
    "        print(e)\n",
    "\n",
    "l_examples = l_new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7b0cc-4d72-4780-9f3a-0fa25ee3a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanize_number(num: float) -> str:\n",
    "    \"\"\"\n",
    "    Converts a number into a human-readable string with k, M, or B suffixes.\n",
    "    \n",
    "    Args:\n",
    "        num (float): The number to format.\n",
    "    \n",
    "    Returns:\n",
    "        str: Human-readable string representation.\n",
    "    \"\"\"\n",
    "    if num >= 1_000_000_000:\n",
    "        return f\"{num / 1_000_000_000:.1f}B\"\n",
    "    elif num >= 1_000_000:\n",
    "        return f\"{num / 1_000_000:.1f}M\"\n",
    "    elif num >= 1_000:\n",
    "        return f\"{num / 1_000:.1f}k\"\n",
    "    else:\n",
    "        return str(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c8aa3-b7b1-48a7-b0fc-c8811fe9a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_params(model):\n",
    "    if 'gpt' in model:\n",
    "        if 'nano' in model:\n",
    "            return 0\n",
    "        elif 'mini' in model:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "\n",
    "    if 'claude' in model:\n",
    "        if 'haiku' in model:\n",
    "            return 3\n",
    "        elif 'sonnet' in model:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "    \n",
    "    return int(re.search(\"([0-9]+)B\", model).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a412a1f-de17-4d22-b798-12ea8ce0db67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_viz = pd.DataFrame(l_examples)\n",
    "\n",
    "orig_len = len(df_viz)\n",
    "\n",
    "# df_viz = df_viz[df_viz['cot_gt_logprobs'].notna()]\n",
    "\n",
    "new_len = len(df_viz)\n",
    "if orig_len != new_len:\n",
    "    print(f\"Lost {orig_len - new_len} examples from na logprobs\")\n",
    "\n",
    "df_viz['encoding_scheme'] = df_viz['data'].map(lambda x: x['experiment_params']['encoding_scheme'])\n",
    "df_viz['model'] = df_viz['data'].map(lambda x: x['experiment_params']['model'])\n",
    "\n",
    "try:\n",
    "    df_viz['model_size'] = df_viz['model'].map(parse_params)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "df_viz['input_type'] = df_viz['data'].map(lambda x: \"_\".join(x['experiment_name'].split(\"_\")[:2]))\n",
    "\n",
    "df_viz['n_few_shot_examples'] = df_viz['data'].map(lambda x: x['experiment_params'].get('n_few_shot_examples', None))\n",
    "\n",
    "df_viz['Adherence Calculation Method'] = df_viz['encoding_scheme'].map(lambda x: get_deterministic_adherence_fn(x, None) is not None).map({ True: 'deterministic', False: 'Sonnet 4 judge'})\n",
    "\n",
    "try:\n",
    "    df_viz['total_train_tok'] = df_viz['n_total_train_tok'].map(humanize_number)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "df_viz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59da245-a8a4-482d-b5a8-814c663f4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz['input_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d5f60-3ce3-448f-b887-0d0723e8a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_set = ['mathcot_fewshot']\n",
    "# filter_set = ['math_cot']\n",
    "# filter_set = ['mathcot_prompted']\n",
    "filter_set = ['mathcot_prompteddecode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294af9ce-482f-4336-a8bc-6aaf90027383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp = df_viz[df_viz['input_type'].isin(filter_set)]\n",
    "\n",
    "df_viz_tmp = df_viz_tmp.sort_values([\n",
    "    'model_size',\n",
    "    'adherent_and_correct'\n",
    "])\n",
    "\n",
    "df_viz_tmp = df_viz_tmp.astype({'n_few_shot_examples': str})\n",
    "\n",
    "df_viz_tmp['encoding_scheme'] = df_viz_tmp['encoding_scheme'].map(lambda s: s.split(\"speaking_\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559e93b-ed7e-473a-b1e1-e40b2d723072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "\n",
    "fig = px.bar(df_viz_tmp_plot, x='encoding_scheme', y='generated_cot_adhered_encoding_style',\n",
    "             height=600, width=1600,\n",
    "             # height=1600, width=1600,\n",
    "             # color='model',\n",
    "             error_y='generated_cot_adhered_encoding_style_hi_ci',\n",
    "             error_y_minus='generated_cot_adhered_encoding_style_low_ci',\n",
    "             # color='n_few_shot_examples',\n",
    "             # facet_row='model',\n",
    "             color='model',\n",
    "             facet_col='Adherence Calculation Method',\n",
    "             # color='training_augmentation',\n",
    "             # color='total_train_tok',\n",
    "             # color_discrete_map=color_discrete_map,\n",
    "             title=\"MATH 500 CoT encoding style adherence\",\n",
    "             barmode=\"group\",\n",
    "             template=\"plotly_white\",\n",
    "             color_discrete_sequence=px.colors.qualitative.Set2,\n",
    "            )\n",
    "\n",
    "fig.update_xaxes(title=\"Encoding scheme\", tickangle=90)\n",
    "fig.update_yaxes(title=\"% adherent encodings\", dtick=0.1)\n",
    "\n",
    "# ✅ Make each facet's x-axis independent to avoid empty slots\n",
    "fig.for_each_xaxis(lambda ax: ax.update(matches=None, categoryorder='trace'))\n",
    "\n",
    "# ✅ Align all x-axis titles by setting a fixed vertical offset\n",
    "ct = [0]\n",
    "def ax_standoff_updater(ax, ct):    \n",
    "    ax.title.update(standoff=ct[0] * 130)\n",
    "    ct[0] += 1\n",
    "\n",
    "fig.for_each_xaxis(lambda ax: ax_standoff_updater(ax, ct))\n",
    "fig.update_yaxes(title_standoff=5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557fec6-ac52-467e-8e7d-afe4b4e5757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def make_encoding_scheme_bar_plot(\n",
    "    df,\n",
    "    y_col=\"generated_cot_is_correct\",\n",
    "    title=\"MATH 500 Accuracy\",\n",
    "    y_axis_title=\"Accuracy\",\n",
    "    x_col=\"encoding_scheme\",\n",
    "    model_col=\"model\",\n",
    "    d_mapping=None,\n",
    "    yaxis_dtick=0.1,            # 10% ticks read cleaner\n",
    "    show_text=False,            # optionally show % text on bars\n",
    "    model_order=None,           # lock legend/order of models\n",
    "    font_family=\"Inter, Arial\", # consistent, modern\n",
    "    sort_by_col=None,\n",
    "    sort_agg=\"max\",\n",
    "    sort_desc=True\n",
    "):\n",
    "    # --- 1) DEFAULT SECTION MAP ---\n",
    "    if d_mapping is None:\n",
    "        d_mapping = {\n",
    "            \"baseline\": [\"zero_shot\", \"identity\"],\n",
    "            # \"letter mutation\": [\n",
    "            #     \"reverse_letters_in_each_word\",\n",
    "            #     \"swap_even_odd_letters_in_each_word\",\n",
    "            #     \"reverse_fibonacci_indices_in_each_word\",\n",
    "            #     \"letter_to_word_with_dot\",\n",
    "            #     \"dot_between_chars\",\n",
    "            #     \"space_between_chars\",\n",
    "            # ],\n",
    "            # \"language deletion\": [\"remove_all_verbs\", \"remove_all_nouns\"],\n",
    "            # \"language translation\": [\n",
    "            #     \"French\",\"Chinese\",\"Korean\",\"Russian\",\"Arabic\",\"Adyghe\",\n",
    "            #     \"Morse_code\",\"Python\",\"enterprise_Java\",\n",
    "            # ],\n",
    "            # \"algorithmic cipher\": [\n",
    "            #     \"rot13_cipher\",\"base64_cipher\",\"base64_2x_cipher\",\n",
    "            #     \"base64_3x_cipher\",\"caesar_cipher\",\"gzip_to_base64_encoded\",\n",
    "            # ],\n",
    "            \"themed reasoning\": [\n",
    "                \"paraphrase_naive\",\n",
    "                \"pirate_speak\",\n",
    "                \"leet_speak\",\n",
    "                \"yoda_speak\",\n",
    "                \"shakespearean_text\",\n",
    "            ],\n",
    "            \"extraneous content\": [\n",
    "                \"insert_tweet\",\n",
    "                \"python_snippet_comment\",\n",
    "                \"croissant_news_article\",\n",
    "                \"math_textbook_article\",\n",
    "                \"five_emojis\",\n",
    "            ],\n",
    "            \"delete inf.\": [\n",
    "                \"replace_math_content_with_black_box\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    df_plot = df.copy()\n",
    "\n",
    "     # ===================== NEW SORTING LOGIC =====================\n",
    "    # Decide which column drives sorting\n",
    "    metric_col = sort_by_col or y_col\n",
    "\n",
    "    # Pick aggregation\n",
    "    agg_map = {\"max\": \"max\", \"mean\": \"mean\", \"median\": \"median\"}\n",
    "    agg_fn = agg_map.get(sort_agg, \"max\")\n",
    "\n",
    "    # Aggregate per scheme across models on the sorting metric\n",
    "    if metric_col not in df_plot.columns:\n",
    "        raise ValueError(f\"sort_by_col '{metric_col}' not found in dataframe.\")\n",
    "\n",
    "    sort_metric = (\n",
    "        df_plot\n",
    "        .groupby(x_col, as_index=False)[metric_col]\n",
    "        .agg(agg_fn)\n",
    "        .rename(columns={metric_col: \"sort_value\"})\n",
    "    )\n",
    "    # =============================================================\n",
    "\n",
    "    # --- 2) ORDERING BY SECTION, THEN BY MAX(Y) DESC ---\n",
    "    full_category_order, section_spans = [], []\n",
    "    present_set = set(df_plot[x_col].unique())\n",
    "    cursor = 0\n",
    "\n",
    "    for section_name, schemes in d_mapping.items():\n",
    "        present = [s for s in schemes if s in present_set]\n",
    "        if not present:\n",
    "            continue\n",
    "\n",
    "        section_df = sort_metric[sort_metric[x_col].isin(present)].copy()\n",
    "        section_df = section_df.sort_values(\"sort_value\", ascending=not sort_desc)\n",
    "        ordered = section_df[x_col].tolist()\n",
    "        if not ordered:\n",
    "            continue\n",
    "\n",
    "        start = cursor\n",
    "        full_category_order.extend(ordered)\n",
    "        cursor = len(full_category_order)\n",
    "        end = cursor - 1\n",
    "        section_spans.append((start, end, section_name))\n",
    "\n",
    "    if not full_category_order:\n",
    "        full_category_order = list(df_plot[x_col].unique())\n",
    "        section_spans = [(0, len(full_category_order) - 1, \"All\")]\n",
    "\n",
    "    df_plot[x_col] = pd.Categorical(df_plot[x_col], categories=full_category_order, ordered=True)\n",
    "\n",
    "    # --- 1) PRESERVE MODEL ORDER FROM DATAFRAME ---\n",
    "    # Use the order models appear in df instead of sorting alphabetically\n",
    "    if model_order is None:\n",
    "        model_order = list(dict.fromkeys(df_plot[model_col]))  # <-- preserves original order\n",
    "    df_plot[model_col] = pd.Categorical(df_plot[model_col], categories=model_order, ordered=True)\n",
    "\n",
    "    # --- 3) ERROR BARS (auto-detect) ---\n",
    "    err_hi_col = f\"{y_col}_hi_ci\"\n",
    "    err_lo_col = f\"{y_col}_low_ci\"\n",
    "    error_y = err_hi_col if err_hi_col in df_plot.columns else None\n",
    "    error_y_minus = err_lo_col if err_lo_col in df_plot.columns else None\n",
    "\n",
    "    def wrap_string(s: str, width: int = 15) -> str:\n",
    "        \"\"\"\n",
    "        Wraps the input string so that each line has at most `width` characters.\n",
    "        If a word exceeds the width, a '-' and newline are inserted.\n",
    "    \n",
    "        Args:\n",
    "            s (str): Input string to wrap.\n",
    "            width (int): Maximum number of characters per line. Default is 15.\n",
    "    \n",
    "        Returns:\n",
    "            str: The wrapped string.\n",
    "        \"\"\"\n",
    "        result = \"\"\n",
    "        i = 0\n",
    "    \n",
    "        while i < len(s):\n",
    "            # Take a chunk of 'width' characters\n",
    "            chunk = s[i:i+width]\n",
    "            # If the chunk is exactly width long and not at the end, add a dash\n",
    "            if len(chunk) == width and i + width < len(s):\n",
    "                result += chunk + \"<br>\"\n",
    "            else:\n",
    "                result += chunk\n",
    "            i += width\n",
    "    \n",
    "        return result\n",
    "\n",
    "    # Label prettifier\n",
    "    def prettify(s: str) -> str:\n",
    "        s = s.replace(\"_\", \" \")\n",
    "        return wrap_string(s, 8)\n",
    "\n",
    "    # --- 2) DETECT WHETHER Y-VALUES ARE PERCENTAGES OR NUMERICAL ---\n",
    "    y_min, y_max = df_plot[y_col].min(), df_plot[y_col].max()\n",
    "    is_percent = y_max <= 1.0 and y_min >= 0.0\n",
    "\n",
    "    # Set default dtick depending on scale\n",
    "    if yaxis_dtick is None:\n",
    "        yaxis_dtick = 0.1 if is_percent else None\n",
    "    \n",
    "    # --- 2) BASE FIGURE (no change here, just ensures model order applies)\n",
    "    fig = px.bar(\n",
    "        df_plot,\n",
    "        x=x_col,\n",
    "        y=y_col,\n",
    "        color=model_col,\n",
    "        category_orders={x_col: full_category_order, model_col: model_order},\n",
    "        barmode=\"group\",\n",
    "        template=\"plotly_white\",\n",
    "        color_discrete_sequence=px.colors.qualitative.Safe,\n",
    "        title=title,\n",
    "        height=600,\n",
    "        width=1800,\n",
    "        error_y=error_y,\n",
    "        error_y_minus=error_y_minus,\n",
    "        text=(df_plot[y_col] if show_text else None),\n",
    "    )\n",
    "\n",
    "    # Y axis as percent\n",
    "    if is_percent:\n",
    "        fig.update_yaxes(\n",
    "            title=y_axis_title,\n",
    "            tickformat=\".0%\",\n",
    "            tickmode=\"linear\",\n",
    "            dtick=yaxis_dtick,\n",
    "            rangemode=\"tozero\"\n",
    "        )\n",
    "    else:\n",
    "        fig.update_yaxes(\n",
    "            title=y_axis_title,\n",
    "            tickmode=\"linear\",\n",
    "            rangemode=\"tozero\",\n",
    "            dtick=yaxis_dtick\n",
    "        )\n",
    "\n",
    "    # X axis: rely on categoryarray for order; set readable tick labels\n",
    "    fig.update_xaxes(\n",
    "        title=\"Encoding scheme\",\n",
    "        ticktext=[f\"<b>{prettify(lbl)}</b>\" for lbl in full_category_order],\n",
    "        tickvals=full_category_order,       # use category values, not numeric indices\n",
    "        tickangle=0\n",
    "    )\n",
    "\n",
    "    # Bar spacing and text formatting\n",
    "    fig.update_traces(\n",
    "        texttemplate = \"%{y:.0%}\" if (is_percent and show_text) else \"%{y:}\" if show_text else None,\n",
    "        textposition=\"outside\" if show_text else \"none\",\n",
    "        # Pass extra fields if you want them in hover (optional)\n",
    "        customdata=df_plot[[model_col]].values,\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        bargap=0.15,\n",
    "        bargroupgap=0.05,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.08,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01,\n",
    "            font=dict(size=10)  # ↓ Smaller legend font\n",
    "        ),\n",
    "        margin=dict(t=110, r=30, b=80, l=70),\n",
    "        font=dict(family=font_family, size=14),\n",
    "        title=dict(font=dict(size=22))\n",
    "    )\n",
    "    \n",
    "    # Make x-axis tick labels smaller\n",
    "    fig.update_xaxes(\n",
    "        ticktext=[f\"<b>{prettify(lbl)}</b>\" for lbl in full_category_order],\n",
    "        tickvals=full_category_order,\n",
    "        tickangle=0,\n",
    "        tickfont=dict(size=10)  # ↓ Smaller x-axis font size\n",
    "    )\n",
    "\n",
    "    # --- 5) SECTION BACKGROUNDS + LABELS ---\n",
    "    N = len(full_category_order)\n",
    "\n",
    "    # Helper: convert [start,end] (category indices) to x domain [0,1]\n",
    "    def to_xdomain(idx):\n",
    "        # position of left edge of category idx:\n",
    "        return idx / N\n",
    "\n",
    "    # alternating light bands for sections\n",
    "    shapes = []\n",
    "    annotations = []\n",
    "    for i, (start_idx, end_idx, name) in enumerate(section_spans):\n",
    "        x0 = to_xdomain(start_idx)\n",
    "        x1 = to_xdomain(end_idx + 1)\n",
    "        band = dict(\n",
    "            type=\"rect\",\n",
    "            xref=\"x domain\", yref=\"paper\",\n",
    "            x0=x0, x1=x1, y0=0, y1=1,\n",
    "            layer=\"below\",\n",
    "            line=dict(width=0),\n",
    "            fillcolor=\"rgba(0,0,0,0.03)\" if i % 2 == 0 else \"rgba(0,0,0,0.00)\"\n",
    "        )\n",
    "        shapes.append(band)\n",
    "        annotations.append(dict(\n",
    "            x=(x0 + x1) / 2, xref=\"x domain\",\n",
    "            y=1.06, yref=\"paper\",\n",
    "            text=name, showarrow=False,\n",
    "            font=dict(size=12, color=\"black\"),\n",
    "            xanchor=\"center\"\n",
    "        ))\n",
    "    fig.update_layout(shapes=shapes)\n",
    "    for a in annotations:\n",
    "        fig.add_annotation(**a)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72f4a6-5246-4397-890a-3eb3ded732a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_viz_tmp,\n",
    "    # df=df_viz_tmp[df_viz_tmp['encoding_scheme'] == 'letter_to_word_with_dot'],\n",
    "    y_col=\"generated_cot_is_correct\",\n",
    "    title=\"MATH 500 Accuracy\",\n",
    "    y_axis_title=\"Accuracy\",\n",
    "    yaxis_dtick=0.05,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    ")\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a704022-7b09-456f-8dc3-81ce9ae100c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_viz_tmp,\n",
    "    y_col=\"adherent_and_correct\",\n",
    "    title=\"Proportion of encoding adherent & correct responses on MATH 500\",\n",
    "    y_axis_title=\"% of responses\",\n",
    "    yaxis_dtick=0.05,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    "\n",
    ")\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a9834-5687-456c-9fce-fe40c5e299b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgr[df_pgr['model'].str.contains('7B') & ~df_pgr['model'].str.contains('Instruct')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b86a2e-c97f-4ee2-ad57-1aef0c0e4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute PGR (percent of identity performance) ---\n",
    "df_pgr = df_viz_tmp.copy()\n",
    "\n",
    "# Identity baseline per model\n",
    "identity_baseline = (\n",
    "    df_pgr[df_pgr[\"encoding_scheme\"] == \"identity\"]\n",
    "    .set_index(\"model\")[\"adherent_and_correct\"]\n",
    ")\n",
    "\n",
    "df_pgr[\"identity_value\"] = np.maximum(df_pgr[\"model\"].map(identity_baseline), 0.0001)\n",
    "\n",
    "# Avoid divide-by-zero\n",
    "# df_pgr = df_pgr[df_pgr[\"identity_value\"] > 0].copy()\n",
    "\n",
    "# Mean PGR as percentage\n",
    "df_pgr[\"PGR_pct\"] = (df_pgr[\"adherent_and_correct\"] / df_pgr[\"identity_value\"])\n",
    "\n",
    "# CI deltas -> percentage deltas relative to identity baseline\n",
    "# low is negative, high is positive\n",
    "df_pgr[\"PGR_pct_hi_ci\"]  = (df_pgr[\"adherent_and_correct_hi_ci\"] / df_pgr[\"identity_value\"])\n",
    "df_pgr[\"PGR_pct_low_ci\"] = (df_pgr[\"adherent_and_correct_low_ci\"]        / df_pgr[\"identity_value\"])\n",
    "\n",
    "df_pgr.loc[df_pgr[\"encoding_scheme\"] == \"identity\", [\"PGR_pct_hi_ci\", \"PGR_pct_low_ci\"]] = 0.0\n",
    "\n",
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_pgr,\n",
    "    y_col=\"PGR_pct\",\n",
    "    title=\"Relative % of responses adherent & correct vs. identity encoding\",\n",
    "    y_axis_title=\"% of identity performance\",\n",
    "    yaxis_dtick=0.1,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    ")\n",
    "fig.show('png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb65175-68f3-4405-b842-e8bf48e3e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_viz_tmp,\n",
    "    y_col=\"cot_gt_logprobs\",\n",
    "     title=\"Log loss on MATH 500 ground-truth encoded CoT from PRM800K\",\n",
    "    y_axis_title=\"Mean log loss per sequence\",\n",
    "    yaxis_dtick=250,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644330e7-ca98-47a0-8883-6e2d35183649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "\n",
    "df_viz_tmp_plot = df_viz_tmp_plot[df_viz_tmp_plot['encoding_scheme'] != 'zero_shot']\n",
    "\n",
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_viz_tmp_plot,\n",
    "    y_col=\"backtranslation_bleu_scores\",\n",
    "     title=\"Encoded -> English translation BLEU\",\n",
    "    y_axis_title=\"Encoded -> English translation BLEU\",\n",
    "    yaxis_dtick=10,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    "\n",
    ")\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f46b8d-efe4-47b3-ba32-51c6a5e28575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "\n",
    "df_viz_tmp_plot = df_viz_tmp_plot[df_viz_tmp_plot['encoding_scheme'] != 'zero_shot']\n",
    "\n",
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_viz_tmp_plot,\n",
    "    y_col=\"total_translation_loss\",\n",
    "     title=\"Encoded -> English translation log loss\",\n",
    "    y_axis_title=\"Mean log loss per sequence\",\n",
    "    yaxis_dtick=50,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    "\n",
    ")\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a837441-3b1d-4760-9fa4-6575058468c0",
   "metadata": {},
   "source": [
    "# Plot the translation ability -> acc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6dbd1-8f91-4387-abb2-e162bbd99345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def styled_logistic_scatter_faceted(\n",
    "    df,\n",
    "    facet_col=\"model\",\n",
    "    x_col=\"backtranslation_gt_logprobs\",\n",
    "    y_col=\"adherent_and_correct\",\n",
    "    title=\"Adherence vs. Backtranslation Log-Prob\",\n",
    "    x_axis_title=\"Backtranslation log-prob\",\n",
    "    y_axis_title=\"Adherent & correct\",\n",
    "    font_family=\"Inter, Arial\",\n",
    "    marker_size=7,\n",
    "    opacity=0.6,\n",
    "    legend_font_size=10,\n",
    "    x_tick_font_size=10,\n",
    "    line_width=3,\n",
    "    r2_digits=3,               # <— precision for R² display\n",
    "    keep_subplot_title=True,\n",
    "    max_pow_override=None\n",
    "):\n",
    "    def logistic(x, L, k, x0):\n",
    "        return L / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "    # overall y-scale to decide percent formatting\n",
    "    y_all = df[y_col].to_numpy(dtype=float)\n",
    "    is_percent = (np.nanmin(y_all) >= 0) and (np.nanmax(y_all) <= 1.0)\n",
    "\n",
    "    # facet ordering\n",
    "    if pd.api.types.is_categorical_dtype(df[facet_col]):\n",
    "        facet_values = [c for c in df[facet_col].cat.categories if (df[df[facet_col]==c].shape[0] > 0)]\n",
    "    else:\n",
    "        d_model_to_size = dict(zip(df['model'], df['model_size']))\n",
    "        facet_values = sorted(df[facet_col].dropna().unique(), key=lambda x: d_model_to_size.get(x, x))\n",
    "\n",
    "    print(facet_values)\n",
    "\n",
    "    n_cols = max(1, len(facet_values))\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=n_cols, shared_yaxes=True, horizontal_spacing=0.06,\n",
    "        subplot_titles=[f\"{facet_col} = {v}\" if keep_subplot_title else None for v in facet_values],\n",
    "    )\n",
    "\n",
    "    x_positive = df[df[x_col] > 0][x_col]\n",
    "    min_x, max_x = x_positive.min(), x_positive.max()\n",
    "    # Create tick values at decade intervals\n",
    "    min_pow = int(np.floor(np.log2(min_x)))\n",
    "    max_pow = int(np.ceil(np.log2(max_x)))\n",
    "    tick_vals = [2 ** p for p in range(min_pow, max_pow + 1)]\n",
    "    if max_pow_override:\n",
    "        if tick_vals[-1] > max_pow_override:\n",
    "            tick_vals[-1] = max_pow_override\n",
    "    tick_text = [f\"{v:g}\" for v in tick_vals]\n",
    "\n",
    "    for i, val in enumerate(facet_values, start=1):\n",
    "        sub = df[df[facet_col] == val]\n",
    "        x = sub[x_col].to_numpy(dtype=float)\n",
    "        y = sub[y_col].to_numpy(dtype=float)\n",
    "\n",
    "        # points\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x, y=y, mode=\"markers+text\", name=f\"Points ({val})\",\n",
    "                marker=dict(size=marker_size, line=dict(width=0)),\n",
    "                opacity=opacity, showlegend=False,\n",
    "                # text=sub[\"encoding_scheme\"] + \"-\" + sub[\"model\"],\n",
    "                hovertemplate=\"<b>%{text}</b><extra></extra>\",  # <-- Just encoding scheme\n",
    "            ),\n",
    "            row=1, col=i\n",
    "        )\n",
    "\n",
    "        # choose regressor space\n",
    "        # linear_on_log_x = True\n",
    "        # if linear_on_log_x:\n",
    "        #     valid = (x > 0) & np.isfinite(x) & np.isfinite(y)\n",
    "        #     X = np.log2(x[valid])\n",
    "        #     Y = y[valid]\n",
    "        # else:\n",
    "        #     valid = np.isfinite(x) & np.isfinite(y)\n",
    "        #     X = x[valid]\n",
    "        #     Y = y[valid]\n",
    "\n",
    "        # if X.size >= 2 and np.nanstd(Y) > 0:\n",
    "        #     b1, b0 = np.polyfit(X, Y, 1)   # Y = b1*X + b0\n",
    "        #     # make a smooth line across current x-range\n",
    "        #     x_line = np.linspace(np.nanmin(x[valid]), np.nanmax(x[valid]), 300)\n",
    "        #     X_line = np.log2(x_line) if linear_on_log_x else x_line\n",
    "        #     y_line = b1 * X_line + b0\n",
    "\n",
    "        #     # R^2 on observed points (in same space used to fit)\n",
    "        #     y_hat = b1 * X + b0\n",
    "        #     ss_res = np.nansum((Y - y_hat) ** 2)\n",
    "        #     ss_tot = np.nansum((Y - np.nanmean(Y)) ** 2)\n",
    "        #     if ss_tot > 0 and np.isfinite(ss_res):\n",
    "        #         r2 = 1 - ss_res / ss_tot\n",
    "        #         r2_text = f\"{r2:.{r2_digits}f}\"\n",
    "\n",
    "        #     fig.add_trace(\n",
    "        #         go.Scatter(\n",
    "        #             x=x_line, y=y_line, mode=\"lines\",\n",
    "        #             name=f\"Linear fit ({val})\",\n",
    "        #             # line=linear_line_kwargs,\n",
    "        #             showlegend=(i == 1),\n",
    "        #         ),\n",
    "        #         row=1, col=i\n",
    "        #     )\n",
    "        \n",
    "        # logistic fit + R^2\n",
    "        r2_text = \"—\"\n",
    "        try:\n",
    "            if np.isfinite(x).sum() >= 3 and np.nanstd(y) > 0:\n",
    "                p0 = [np.nanmax(y), 1.0, np.nanmedian(x)]\n",
    "                params, _ = curve_fit(logistic, x, y, p0=p0, maxfev=10000)\n",
    "                L, k, x0 = params\n",
    "\n",
    "                x_fit = np.linspace(np.nanmin(x), max(np.nanmax(x), 98), 300)\n",
    "                y_fit = logistic(x_fit, L, k, x0)\n",
    "\n",
    "                # R^2 on observed x\n",
    "                y_hat = logistic(x, L, k, x0)\n",
    "                ss_res = np.nansum((y - y_hat) ** 2)\n",
    "                ss_tot = np.nansum((y - np.nanmean(y)) ** 2)\n",
    "                if ss_tot > 0 and np.isfinite(ss_res):\n",
    "                    r2 = 1 - ss_res / ss_tot\n",
    "                    r2_text = f\"{r2:.{r2_digits}f}\"\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=x_fit, y=y_fit, mode=\"lines\",\n",
    "                        name=f\"Logistic fit ({val})\",\n",
    "                        line=dict(width=1, color=\"black\", dash=\"dash\"),\n",
    "                        showlegend=(i == 1),\n",
    "                    ),\n",
    "                    row=1, col=i\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        # R^2 annotation (top-left of this subplot)\n",
    "        fig.add_annotation(\n",
    "            x=0.1, y=0.5,\n",
    "            xref=f\"x{i}\" if i > 1 else \"x\",   # ✅ Use proper axis names\n",
    "            yref=f\"y{i}\" if i > 1 else \"y\",\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"top\",\n",
    "            text=f\"R² = {r2_text}\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=12),\n",
    "            align=\"left\",\n",
    "            bgcolor=\"rgba(255,255,255,0.6)\",\n",
    "            bordercolor=\"rgba(0,0,0,0.2)\",\n",
    "            borderwidth=1,\n",
    "        )\n",
    "\n",
    "        # axes cosmetics\n",
    "        fig.update_xaxes(\n",
    "            title=x_axis_title if i == 1 else \"\",\n",
    "            tickfont=dict(size=x_tick_font_size),\n",
    "            zeroline=False,\n",
    "            type=\"log\",\n",
    "            autorange=\"reversed\",\n",
    "            tickvals=tick_vals,\n",
    "            ticktext=tick_text,\n",
    "            row=1, col=i,\n",
    "            dtick=0.1\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            tickformat=\".0%\" if is_percent else \",\",\n",
    "            rangemode=\"tozero\",\n",
    "            zeroline=False,\n",
    "            dtick=0.05,\n",
    "            row=1, col=i,\n",
    "            title=y_axis_title\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title=dict(text=title, font=dict(size=18)),\n",
    "        height=600,\n",
    "        width=350 * n_cols if n_cols <= 3 else 300 * n_cols,\n",
    "        font=dict(family=font_family, size=14),\n",
    "        margin=dict(t=0, r=0, b=0, l=0),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.08, xanchor=\"left\", x=0,\n",
    "                    font=dict(size=legend_font_size)),\n",
    "        showlegend=False\n",
    "    )\n",
    "    # global axis labels\n",
    "    # fig.add_annotation(text=x_axis_title, showarrow=False, xref=\"paper\", yref=\"paper\",\n",
    "    #                    x=0.5, y=-0.18, font=dict(size=14))\n",
    "    # fig.add_annotation(text=y_axis_title, showarrow=False, xref=\"paper\", yref=\"paper\",\n",
    "    #                    x=-0.06, y=0.5, textangle=-90, font=dict(size=14))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ---- Use it on your dataframe ----\n",
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "\n",
    "start_len = len(df_viz_tmp_plot)\n",
    "df_viz_tmp_plot = df_viz_tmp_plot[~df_viz_tmp_plot['encoding_scheme'].isin(['identity', 'zero_shot'])]\n",
    "assert len(df_viz_tmp_plot) != start_len\n",
    "\n",
    "df_viz_tmp_plot[\"placeholder_col\"] = \"1\"\n",
    "\n",
    "d_zero_shot_baseline =  (\n",
    "    df_pgr[df_pgr[\"encoding_scheme\"] == \"zero_shot\"]\n",
    "    .set_index(\"model\")[\"adherent_and_correct\"]\n",
    ")\n",
    "\n",
    "fig = styled_logistic_scatter_faceted(\n",
    "    # df_viz_tmp_plot[df_viz_tmp_plot['backtranslation_bleu_scores'].notna() & ~df_viz_tmp_plot['model'].str.contains('gpt')],\n",
    "    #[df_viz_tmp_plot['adherent_and_correct'] > 0.01],\n",
    "    df_viz_tmp_plot[(df_viz_tmp_plot['adherent_and_correct'] >= df_viz_tmp_plot['model'].map(d_zero_shot_baseline)) & (~df_viz_tmp_plot['model'].str.contains('gpt'))],\n",
    "    x_col=\"backtranslation_gt_logprobs\",\n",
    "    # x_col=\"backtranslation_bleu_scores\",\n",
    "    y_col=\"adherent_and_correct\",\n",
    "    # y_col='cot_gt_logprobs',\n",
    "    # title=\"% of responses adherent & correct vs. encoded -> English translation BLEU\",\n",
    "    title=None,\n",
    "    # x_axis_title=\"Encoded text to English text BLEU score\",\n",
    "    x_axis_title=\"Encoded text to English text log loss\",\n",
    "    y_axis_title=\"% of MATH500 responses adherent & correct\",\n",
    "    # facet_col=\"model\",\n",
    "    facet_col=\"placeholder_col\",\n",
    "    keep_subplot_title=False,\n",
    "    max_pow_override=100,\n",
    "    x_tick_font_size=12\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(width=600, height=450, title=None)\n",
    "# fig.update_layout(width=1600, height=1200, title=None)\n",
    "# --- shaded region instead of vline ---\n",
    "# x_cut = 60\n",
    "# x_right = float(df_viz_tmp_plot[\"backtranslation_bleu_scores\"].max()) + 30\n",
    "x_cut = 0.2\n",
    "x_right = 0.02\n",
    "\n",
    "\n",
    "fig.add_vrect(\n",
    "    x0=x_cut, x1=x_right,           # shade from cutoff to the right edge of data\n",
    "    fillcolor=\"#D3D3D3\", opacity=0.25,\n",
    "    line_width=0,\n",
    "    layer=\"below\",\n",
    "    row=1, col=1                    # adjust if you facet into multiple columns\n",
    ")\n",
    "\n",
    "# label for the shaded region\n",
    "fig.add_annotation(\n",
    "    x=0.84,\n",
    "    # x=0.85,\n",
    "    xref=\"paper\",           # data coordinates\n",
    "    y=0.99, yref=\"paper\",\n",
    "    text=\"fluent translation\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"top\",\n",
    "    bgcolor=\"rgba(255,255,255,0.0)\",\n",
    "    bordercolor=\"rgba(0,0,0,0.0)\",\n",
    "    borderwidth=0,\n",
    "    font=dict(size=9)\n",
    ")\n",
    "fig.update_traces(textposition=\"bottom left\")\n",
    "\n",
    "\n",
    "fig.show('png')\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e15b74-d9a1-4718-bdbd-83b80f0b453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test_hashes = ['f01404b75a072276ac1ca6f131313a8a615dbb99', '481e3a2a1f521438c8938721b0df08a389e3406f', 'b411a66e1c2f9ca7a79c065099879f202990c9a2', '33c2595336eb7c74c6820302498fd097fa52463c', 'e3f76d94751a09af21005696ac072219143ebc58', 'b77d4d2334cc5dad12f2168d5fa48749622f34af', '2007b98cc05baeb988564f2e51d6f496a8afe7b8', 'f0a6d8673396c2f4415e10c2fc13effa262fe50d', '16b151e08633744e1c501d6f41707177c5aa6125', 'a846c2161731fabfe9b9d371ac05473c5ba9289d', 'a9a752f3c050a16ef74c6ad9436f0aa6291fb1ff', 'b4c85c27bcc89f916f50867f974bd13875416067']\n",
    "\n",
    "for hash in l_test_hashes:\n",
    "    target_path = f\"/home/ubuntu/sky_workdir/encoding-schemes/output/{hash}/data/sft_train.parquet\"\n",
    "    if os.path.exists(target_path):\n",
    "        print(duckdb.query(f\"SELECT SUM(num_tokens) FROM read_parquet('{target_path}')\").to_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd132a-4f07-4354-8aef-ecb744066d02",
   "metadata": {},
   "source": [
    "# Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcaf2b4-16fb-491c-966f-85ea99452d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoding_schemes import get_deterministic_adherence_fn\n",
    "from encoding_schemes.letter_permutations import get_English_dictionary, reverse_letters_in_each_word, normalize_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea91e7cf-1be3-48f0-be20-ce1918c231b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scheme = 'speaking_letter_to_word_with_dot'\n",
    "test_model = 'Qwen/Qwen2.5-14B-Instruct'\n",
    "test_idx = 21\n",
    "\n",
    "adherence_fn = get_deterministic_adherence_fn(test_scheme, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23894a-6e69-4aa1-af84-dbdab60c1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['reference_problem_df'].iloc[0][test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cecb8de-42aa-4717-9646-64d7528f6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['reference_solution_df'].iloc[0][test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4feb0b5-0009-474e-b920-63453ce01ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['generated_cots_df'].iloc[0][test_idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9a283-2629-4ac5-ba34-b50b90764adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_letters_in_each_word(df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['generated_cots_df'].iloc[0][test_idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f92dd3-24db-4bd9-94cf-c25342c3d6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['generated_cot_adhered_encoding_style'].iloc[0]#[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561fed07-320b-47cf-bedd-c2cf7d2af6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_hash = df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['experiment_hash'].iloc[0]\n",
    "example_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd34c8-c820-4689-9865-b5287e3fa4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/home/ubuntu/sky_workdir/encoding-schemes/output/{example_hash}/data/sft_model_meta.json\", \"r\") as fp:\n",
    "    d_example = json.load(fp)\n",
    "\n",
    "d_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4a42f-b0d6-4c25-a09c-45c80a4e2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_test_path = f\"/home/ubuntu/sky_workdir/encoding-schemes/output/{example_hash}/data/sft_train.parquet\"\n",
    "\n",
    "df_sft = pd.read_parquet(sft_test_path)\n",
    "df_sft['messages'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852ab00-13eb-4aec-88f0-5475f3cb3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23601359-7ca9-4014-a9ef-2622cd58edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft = df_sft.iloc[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ecbbd-d047-4a5c-9923-fa5754e889ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft['num_tokens'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49010c-5082-487b-b74c-d76d9b8d0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sft_parquet_to_jsonl(df, output_json_path):\n",
    "    n_rows_written = 0\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, raw in enumerate(df[\"messages\"]):\n",
    "\n",
    "            json_line = {\"messages\": list(raw)}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + \"\\n\")\n",
    "            n_rows_written += 1\n",
    "\n",
    "    print(f\"[prep] Wrote {n_rows_written} training rows to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e777b-73e2-409d-9443-837fad99fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_sft_parquet_to_jsonl(df_sft, sft_test_path.replace(\"parquet\", \"jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c2b6a-dbdc-4567-a523-0c537a456126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/e4a87a8626efeeb6df76d94bb34e7e5e34c77154/data/sft_train.parquet\")\n",
    "\n",
    "df_sft = df_sft.iloc[:9120]\n",
    "\n",
    "df_sft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2057b7-3657-40c5-ab01-9fa47155aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft['num_tokens'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33d894-e729-4149-95d6-2e3eeb8c0dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
