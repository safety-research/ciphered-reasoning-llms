{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d0a44-0aae-4a22-9ea2-552f1efd9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import duckdb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31791de-bdbf-44f4-ac17-de4f754d0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/sky_workdir/encoding-schemes\")\n",
    "\n",
    "from encoding_schemes import get_deterministic_adherence_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8432f-7bb7-4bb8-8223-ea329456a590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8ad9e-91d3-4e3b-b25c-9f19f3e4b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "conn_string = os.environ[\"SUPABASE_CONNECTION_URL\"]\n",
    "\n",
    "conn = psycopg2.connect(conn_string)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b3a2c-ade1-42ca-a798-5b94232485d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_sql(\"SELECT * FROM public.encoding_schemes WHERE (data->'experiment_tags'->'sft')::boolean\", conn)\n",
    "\n",
    "# sel_str = \"\"\"\n",
    "# -- redo prompted\n",
    "# (\n",
    "#     (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "#     AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "#     AND (data->'experiment_name')::TEXT LIKE '%prompted_%'\n",
    "# )\n",
    "# \"\"\"\n",
    "\n",
    "# sel_str = \"\"\"\n",
    "# -- Few shot\n",
    "#  (\n",
    "#      (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "#      AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "#      AND (\n",
    "#          (data->'experiment_params'->'n_few_shot_examples')::INT = 8\n",
    "#      )\n",
    "#   )\n",
    "# \"\"\"\n",
    "\n",
    "sel_str = \"\"\"\n",
    "-- NuminaMath CoT Rerun\n",
    " (\n",
    "     (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "     AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "     AND (\n",
    "         (data->'experiment_params'->'sampling_params'->'n')::INT = 4\n",
    "         OR ((data->'experiment_params'->'model')::TEXT LIKE '%gpt%' AND (data->'experiment_params'->'sft_params'->'batch_size')::INT != 48)\n",
    "     )\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "# sel_str = \"\"\"\n",
    "# -- NuminaMath CoT Rerun\n",
    "#  (\n",
    "#      (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "#      AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "#      AND (\n",
    "#          (data->'experiment_params'->'model')::TEXT LIKE '%gpt%'\n",
    "#      )\n",
    "#      AND (\n",
    "#         (data->'experiment_params'->'encoding_scheme')::TEXT LIKE '%reverse_letters%'\n",
    "#         OR (data->'experiment_params'->'encoding_scheme')::TEXT LIKE '%identity%'\n",
    "#      )\n",
    "#   )\n",
    "# \"\"\"\n",
    "\n",
    "# sel_str = \"\"\"\n",
    "# -- prompted no sft decode\n",
    "# (\n",
    "#     (data->'experiment_tags'->'numina_math_cot_rerun')::BOOL\n",
    "#     AND (NOT (data->'force_overwrite')::BOOL OR data->'force_overwrite' IS NULL)\n",
    "#     AND (data->'experiment_name')::TEXT LIKE '%prompteddecode%'\n",
    "# )\n",
    "# \"\"\"\n",
    "\n",
    "df = pd.read_sql(f\"\"\"\n",
    "SELECT * FROM public.encoding_schemes \n",
    "    WHERE \n",
    "\n",
    "{sel_str}\n",
    "\n",
    "\n",
    "ORDER BY created_at DESC\n",
    "\"\"\", conn)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439f59a-7755-482d-ac55-c324ac5bed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/ubuntu/sky_workdir/encoding-schemes/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d72f27-7ec8-4ffd-b530-f2b0b27cefab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_examples = df.to_dict('records')\n",
    "\n",
    "l_examples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0754888-dc53-4d5c-a6e4-5c371360a198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[example for example in l_examples if example[\"data\"][\"experiment_params\"]['encoding_scheme'] == 'speaking_base64_cipher' and \"gpt-4.1-2025\" in example[\"data\"][\"experiment_params\"][\"model\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af510d9b-d9e1-4aea-ab11-c2764186502c",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf91ea-cb39-4cc7-890c-7806eca0d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_acc = []\n",
    "\n",
    "for temp in [0.001, 0.1, 0.3, 0.5, 0.7, 1.0]:\n",
    "    df_fewshot = pd.read_parquet(f\"/home/ubuntu/sky_workdir/encoding-schemes/output/27888d7de3b05cca5d2645c21cc83ffda26650ee/data/temperature_{temp}/math_scores.parquet\") \n",
    "\n",
    "\n",
    "    l_acc.append(np.mean(df_fewshot['is_corrects'].tolist()).flatten())\n",
    "\n",
    "l_acc, [0.001, 0.1, 0.3, 0.5, 0.7, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013ab9c-ad7f-4a4a-a9d5-8642787be55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(y=np.array(l_acc).flatten(), x=[0.001, 0.1, 0.3, 0.5, 0.7, 1.0])\n",
    "\n",
    "fig.update_yaxes(title=\"MATH 500 accuracy\")\n",
    "fig.update_xaxes(title=\"temperature\")\n",
    "fig.update_layout(height=600, width=800, title=\"Temperature sweep for Qwen2.5 14B, reverse letters in each word\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd584f70-0f71-47dc-a5ec-c90c5bb96894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"GAIR/LIMO\")\n",
    "\n",
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507e705-2472-43c7-a20b-c42cc51f96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct\")\n",
    "\n",
    "n_total_tok = 0\n",
    "\n",
    "for row in ds['train']:\n",
    "    n_total_tok += len(tokenizer.encode(row['question']))\n",
    "    n_total_tok += len(tokenizer.encode(row['solution']))\n",
    "\n",
    "n_total_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d284f-3fd8-468f-b175-e2030fa605f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"simplescaling/s1K\")\n",
    "\n",
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea43e9-05b7-45a8-bbad-5a218dc65a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_tok = 0\n",
    "\n",
    "for row in ds['train']:\n",
    "    n_total_tok += len(tokenizer.encode(row['question']))\n",
    "    n_total_tok += len(tokenizer.encode(row['thinking_trajectories'][0]))\n",
    "\n",
    "n_total_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5695bf78-e870-4f58-9fa2-770d0bea94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/6bedc631358628ba4327951c3ab1753788e886bf/data/exit0/math_scores.parquet\")\n",
    "np.mean(list(df_fewshot['is_corrects']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e17ae2-e847-475f-875c-f6aee01d5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/6bedc631358628ba4327951c3ab1753788e886bf/data/exit1/math_scores.parquet\")\n",
    "np.mean(list(df_fewshot['is_corrects']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2d5e2-944b-4443-b16c-ffd46289831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/6bedc631358628ba4327951c3ab1753788e886bf/data/exit2/math_scores.parquet\")\n",
    "np.mean(list(df_fewshot['is_corrects']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc3633-d01c-44ca-9ebb-fb38d061d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/6bedc631358628ba4327951c3ab1753788e886bf/data/final/math_scores.parquet\")\n",
    "np.mean(list(df_fewshot['is_corrects']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a8d526-91f1-44ee-aa73-37c3ace061f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/66525cd7fd3d441670d728733520fec9f16a8dd0/data/joined_output.parquet\") # rot13\n",
    "# df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/fa6964749b4f5e04200981cf46cc85a15b22cdfc/data/joined_output.parquet\") # reverse letters in each word\n",
    "# df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/7b88a15cc7e51bd9ce792e47c494d2530577f989/data/joined_output.parquet\") # base64\n",
    "# df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/19a13b8d5227b89e30470dfbec5588029e6cbb8a/data/joined_output.parquet\") # identity sonnet 35\n",
    "# df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/d3bd4346e68ace0f3ea5eb3d0a05730e42ea2281/data/length_control/math_scores.parquet\") \n",
    "# df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/d3bd4346e68ace0f3ea5eb3d0a05730e42ea2281/data/math_scores.parquet\") \n",
    "# df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/7b350b992b24426e6c5b9de2892e29b4736dc730/data/final/math_scores.parquet\")\n",
    "df_fewshot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/6bedc631358628ba4327951c3ab1753788e886bf/data/final/math_scores.parquet\")\n",
    "\n",
    "\n",
    "df_fewshot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf51be-64bc-4b29-8481-d186b16a9c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoding_schemes.letter_permutations import reverse_letters_in_each_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa5d4b-96fe-48c8-8a5d-2efa59ce3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(df_fewshot['is_corrects']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f7bac-e012-4c46-98bf-40746a81b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_letters_in_each_word(df_fewshot['model_cot'].iloc[8][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570787c1-e5cd-4a86-bcf4-200fd3098bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12687ddd-098f-474d-bd2e-72286a9e5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_letters_in_each_word(df_fewshot['model_cot'].iloc[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d92929-8e74-4c8d-8a87-1c979b618f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/7b88a15cc7e51bd9ce792e47c494d2530577f989/data/sft_train.parquet\") # base64\n",
    "\n",
    "len(df_sft), df_sft['num_tokens'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca4d01-9ad5-4406-b08f-18fc183e8c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79a822-eb8c-4c6d-8a6e-e3f0c4142715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize & get output lens\n",
    "\n",
    "df_fewshot['n_cot_tokens'] = df_fewshot['model_cot'].map(lambda x: [len(tokenizer.encode(s)) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ede436-41c8-47ec-a8a4-6c07bf5824c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tokens = np.array(df_fewshot['n_cot_tokens'].tolist()).flatten()\n",
    "l_is_correct = np.array(df_fewshot['is_corrects'].tolist()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce0a41-a88a-43cd-92b9-74388a379987",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l_tokens), len(l_is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1bc3e-41b7-46a4-8d86-cbd345700e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_token_buckets = [int(n_tok // 200) * 200 for n_tok in l_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64fc57-69a6-4102-8ebb-1f6224c2dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = pd.DataFrame({'is_correct': l_is_correct, 'n_tokens': l_tokens})\n",
    "\n",
    "df_query = duckdb.query(\"SELECT CAST(n_tokens / 200 AS INT) * 200 as n_token_bucket, AVG(is_correct) as is_correct_pct, COUNT(*) as ct FROM df_query WHERE n_token_bucket < 10000 GROUP BY n_token_bucket HAVING COUNT(*) > 50\").to_df()\n",
    "\n",
    "fig = px.bar(df_query, x='n_token_bucket', y='is_correct_pct', width=800, height=600, title=\"Length sweep, reverse_letters_in_each_word_no_math_expressions\")#, color='ct')\n",
    "\n",
    "fig.update_xaxes(dtick=200, title=\"# tokens (bucket)\")\n",
    "fig.update_yaxes(dtick=0.05, title=\"MATH 500 accuracy\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360749e3-5bbb-43c6-8f0c-7ac837dc6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_difficulty_controlled = []\n",
    "\n",
    "for _, row in df_fewshot.iterrows():\n",
    "    sorted_indices = np.argsort(row['n_cot_tokens'])\n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        l_difficulty_controlled.append({\n",
    "            'n_tokens': row['n_cot_tokens'][idx],\n",
    "            'bucket': i,\n",
    "            'is_correct': row['is_corrects'][idx]\n",
    "        })\n",
    "df_difficulty_controlled = pd.DataFrame(l_difficulty_controlled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918931b-e6d9-477c-b193-6e8ee7d5f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = duckdb.query(\"SELECT bucket, AVG(is_correct) as is_correct_pct, COUNT(*) as ct FROM df_difficulty_controlled GROUP BY bucket HAVING COUNT(*) > 50\").to_df()\n",
    "\n",
    "fig = px.bar(df_query, x='bucket', y='is_correct_pct', width=800, height=600, title=\"Effect of length on math accuracy, reverse_letters_in_each_word_no_math_expressions\")#, color='ct')\n",
    "\n",
    "fig.update_xaxes(dtick=1, title=\"# tokens (bucket ordered by token length)\")\n",
    "fig.update_yaxes(dtick=0.05, title=\"MATH 500 accuracy\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4768e28-1d97-46ba-bd91-5ab336785960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_reverse = df_difficulty_controlled.copy()\n",
    "df_query_reverse['encoding_scheme'] = 'reverse_letters_in_each_word_no_math_expressions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe491f1-88c9-4133-bdf7-a4816e623730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefb150-7503-4e94-898a-9baf177b17dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2c0ad-749c-429f-9b21-3b3b6bab226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewshot_identity = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/dd7f9ee4bb4c09a61f663823c4a62158fcda90b5/data/length_control/math_scores.parquet\") \n",
    "\n",
    "df_fewshot_identity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee01f0-fa4a-45aa-8767-d8c6fbd6aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewshot_identity['n_cot_tokens'] = df_fewshot_identity['model_cot'].map(lambda x: [len(tokenizer.encode(s)) for s in x])\n",
    "\n",
    "l_difficulty_controlled = []\n",
    "\n",
    "for _, row in df_fewshot_identity.iterrows():\n",
    "    sorted_indices = np.argsort(row['n_cot_tokens'])\n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        l_difficulty_controlled.append({\n",
    "            'n_tokens': row['n_cot_tokens'][idx],\n",
    "            'bucket': i,\n",
    "            'is_correct': row['is_corrects'][idx]\n",
    "        })\n",
    "df_difficulty_controlled = pd.DataFrame(l_difficulty_controlled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae923e48-7151-4c34-a360-15f14d748184",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tokens = np.array(df_fewshot_identity['n_cot_tokens'].tolist()).flatten()\n",
    "l_is_correct = np.array(df_fewshot_identity['is_corrects'].tolist()).flatten()\n",
    "\n",
    "df_query = pd.DataFrame({'is_correct': l_is_correct, 'n_tokens': l_tokens})\n",
    "\n",
    "df_query = duckdb.query(\"SELECT CAST(n_tokens / 200 AS INT) * 200 as n_token_bucket, AVG(is_correct) as is_correct_pct, COUNT(*) as ct FROM df_query WHERE n_token_bucket < 10000 GROUP BY n_token_bucket HAVING COUNT(*) > 50\").to_df()\n",
    "\n",
    "fig = px.bar(df_query, x='n_token_bucket', y='is_correct_pct', width=800, height=600, title=\"Length sweep, identity\")#, color='ct')\n",
    "\n",
    "fig.update_xaxes(dtick=200, title=\"# tokens (bucket)\")\n",
    "fig.update_yaxes(dtick=0.05, title=\"MATH 500 accuracy\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba99d9-2670-4e79-accb-41bbbe4827b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = duckdb.query(\"SELECT bucket, AVG(is_correct) as is_correct_pct, COUNT(*) as ct FROM df_difficulty_controlled GROUP BY bucket HAVING COUNT(*) > 50\").to_df()\n",
    "\n",
    "fig = px.bar(df_query, x='bucket', y='is_correct_pct', width=800, height=600, title=\"Effect of length on math accuracy, identity\")#, color='ct')\n",
    "\n",
    "fig.update_xaxes(dtick=1, title=\"# tokens (bucket ordered by token length)\")\n",
    "fig.update_yaxes(dtick=0.05, title=\"MATH 500 accuracy\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04193542-2c65-49e7-bbb6-7676593a6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_identity = df_difficulty_controlled.copy()\n",
    "df_query_identity['encoding_scheme'] = 'identity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3420591-3fbb-45e7-820f-213694df8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_difficulty_controlled_combined = pd.concat([df_query_identity, df_query_reverse], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f09bf-1520-4e5f-8cbb-8fa9c09643b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_difficulty_controlled_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742ea2d-d729-45b9-91e6-9e33adc2bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = duckdb.query(\"SELECT CAST(n_tokens / 200 AS INT) * 200 as n_token_bucket, encoding_scheme, AVG(is_correct) as is_correct_pct, COUNT(*) as ct FROM df_difficulty_controlled_combined WHERE n_token_bucket < 10000 GROUP BY n_token_bucket, encoding_scheme HAVING COUNT(*) > 50 ORDER BY n_token_bucket, encoding_scheme\").to_df()\n",
    "\n",
    "fig = px.line(df_query, x='n_token_bucket', y='is_correct_pct', width=1000, height=600, title=\"Token length vs MATH accuracy\", color=\"encoding_scheme\")#, barmode=\"group\")#, color='ct')\n",
    "\n",
    "fig.update_xaxes(dtick=200, title=\"# tokens (bucket)\")\n",
    "fig.update_yaxes(dtick=0.05, title=\"MATH 500 accuracy\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e59e1-98e3-42ad-83ad-665d06e1a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_query = duckdb.query(\"\"\"SELECT bucket, encoding_scheme, AVG(is_correct) as is_correct_pct, \n",
    "AVG(is_correct) / (SELECT AVG(is_correct) FROM df_difficulty_controlled_combined a WHERE a.bucket = 0 AND b.encoding_scheme = a.encoding_scheme) as rel_acc_to_bucket_0,\n",
    "COUNT(*) as ct,\n",
    "AVG(n_tokens) as n_tok\n",
    "FROM df_difficulty_controlled_combined b GROUP BY bucket, encoding_scheme HAVING COUNT(*) > 50\n",
    "ORDER BY bucket, encoding_scheme\n",
    "\"\"\").to_df()\n",
    "\n",
    "fig = px.line(df_query, x='bucket', y='rel_acc_to_bucket_0', color='encoding_scheme', width=1000, height=600, title=\"Effect of length on math accuracy\")\n",
    "            # barmode='group')#, color='ct')\n",
    "\n",
    "fig.update_xaxes(dtick=1, title=\"Rollout bucket (ordered by length)\", tickangle=0)\n",
    "fig.update_yaxes(dtick=0.05, title=\"Accuracy relative to shortest rollout per problem\", tickangle=0)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abcde6-c4b1-4821-818b-54ed99ac506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_query, x='n_tok', y='bucket', color='encoding_scheme', width=1000, height=600, title=\"Bucket length differences\")\n",
    "            # barmode='group')#, color='ct')\n",
    "\n",
    "fig.update_xaxes(title=\"mean # tokens in bucket\", tickangle=0)\n",
    "fig.update_yaxes(dtick=1, title=\"Rollout bucket (ordered by length)\", tickangle=0)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053782d9-5367-4336-b690-9846d493eae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972dd570-a60f-4c84-803b-d6615bbc7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c756dd-4485-4112-892b-5138266f61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewshot.iloc[test_idx]['backtranslation_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc3a55-42b5-4334-9e92-6db956fe404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewshot.iloc[test_idx]['target_backtranslation_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58336b5-ba32-4717-b409-50d3f358b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewshot.iloc[test_idx]['generated_backtranslations'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef482ac-2bcb-46ad-81d0-1a2e523a92e7",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a81ec-f561-430e-8570-c9d4a2a0f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_ci(data, statistic=np.mean, alpha=0.05, n_boot=10_000, random_state=None):\n",
    "    \"\"\"\n",
    "    Returns (point_estimate, low_CI, high_CI) for given 1D data.\n",
    "    Works with bool, int, or float data.\n",
    "    \"\"\"\n",
    "    x = np.asarray(data).astype(float)  # ensure numeric\n",
    "    x = x[~np.isnan(x)]\n",
    "    if len(x) == 0:\n",
    "        raise ValueError(\"No valid data for bootstrapping.\")\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = len(x)\n",
    "\n",
    "    # Draw bootstrap samples\n",
    "    idx = rng.integers(0, n, size=(n_boot, n))\n",
    "    samples = x[idx]\n",
    "\n",
    "    # Apply statistic row-wise\n",
    "    stats = np.apply_along_axis(statistic, 1, samples)\n",
    "\n",
    "    point = statistic(x)\n",
    "    lo = np.percentile(stats, 100 * (alpha / 2))\n",
    "    hi = np.percentile(stats, 100 * (1 - alpha / 2))\n",
    "    return point, lo, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fb9f9-33d1-4e61-b837-cc8b37219291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def count_tokens_from_messages(s):\n",
    "    try:\n",
    "        return len(encoding.encode(s, disallowed_special=()))\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db9fcc-ffa9-4a77-ad34-88067f4d63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=2, memory=32 * 1024 * 1024 * 1024)\n",
    "def compute_translation_token_count(example, df_data):\n",
    "    sys.path.append(\"/home/ubuntu/sky_workdir/encoding-schemes\")\n",
    "\n",
    "    from orchestration.experiment_meta_saver import compute_experiment_hash\n",
    "    from utils.io_utils import read_large_parquet\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = example[\"data\"][\"experiment_params\"][\"model\"]\n",
    "    if \"gpt\" in model or \"claude\" in model:\n",
    "        print(f\"Overriding tokenizer for {model} with gpt-oss 120b tokenizer because it was detected as a GPT/Claude model!\")\n",
    "        model = \"openai/gpt-oss-120b\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "    return df_data['reference_solution'].map(lambda x: len(tokenizer.encode(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f84e71-0f86-4e06-b744-96e3e6a71ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231976a-7a7d-4f2a-90f8-eaa70083c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ci_cols(example, df, col_name, transformation_fn):\n",
    "    s_transformed = transformation_fn(df[col_name])\n",
    "    if np.isscalar(s_transformed) and np.isnan(s_transformed):\n",
    "        print(f\"Warning: {col_name} was all NaN, ignoring!\")\n",
    "        return\n",
    "    mid, lo, hi = bootstrap_ci(s_transformed)\n",
    "    example[col_name] = mid\n",
    "    example[f'{col_name}_low_ci'] = mid - lo\n",
    "    example[f'{col_name}_hi_ci'] = hi - mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1d5fd-3931-4206-a956-9704eae789ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multi_row_transformation(df, row_transform_fn, col_name, agg_fn):\n",
    "    df[col_name] = df.apply(row_transform_fn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73907c00-ce16-4bce-b6e9-14d9bb68e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _score_rollouts(rollouts):\n",
    "    # rollouts: list/array of rollout sequences; may also be None/np.nan/scalar\n",
    "    if rollouts is None or (isinstance(rollouts, float) and np.isnan(rollouts)):\n",
    "        return np.nan\n",
    "\n",
    "    vals = []\n",
    "    for r in rollouts:\n",
    "        # skip None/NaN\n",
    "        if r is None or (isinstance(r, float) and np.isnan(r)):\n",
    "            continue\n",
    "        vals.append(-np.nansum(r))\n",
    "    return np.nanmean(vals) if len(vals) else np.nan\n",
    "\n",
    "\n",
    "def patch_gpt_api_log_loss(example):\n",
    "    experiment_hash = example['experiment_hash']\n",
    "\n",
    "    translation_loss = os.path.join(root_dir, experiment_hash, \"data\", f\"validation_reverse_translation_math500_meta.json\")\n",
    "    with open(translation_loss, \"r\") as fp:\n",
    "        example[\"backtranslation_gt_logprobs\"] = translation_loss[\"valid_loss\"]\n",
    "\n",
    "    # need to be validation loss on 512k...\n",
    "    validation_loss = os.path.join(root_dir, experiment_hash, \"data\", f\"validation_reverse_translation_math500_meta.json\")\n",
    "    with open(validation_loss, \"r\") as fp:\n",
    "        example[\"backtranslation_gt_logprobs\"] = translation_loss[\"valid_loss\"]\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def process_single_example(example):\n",
    "    experiment_hash = example['experiment_hash']\n",
    "    \n",
    "    target_path = os.path.join(root_dir, example['experiment_hash'], \"data\", \"joined_output.parquet\")\n",
    "    if not os.path.exists(target_path):\n",
    "        print(f\"!!!!!! {target_path} missing !!!!!!!\")\n",
    "        return example\n",
    "    \n",
    "    df_data = pd.read_parquet(target_path)\n",
    "\n",
    "    d_col_to_transform = {\n",
    "        'cot_gt_logprobs' : lambda s: np.nansum(s.map(_score_rollouts)),\n",
    "        'generated_cot_is_correct' : np.mean,  # was np.mean\n",
    "        'backtranslation_gt_logprobs' : lambda s: np.nanmean(s.map(_score_rollouts)),\n",
    "        'backtranslation_bleu_scores' : np.mean,  # was np.mean\n",
    "        'generated_cot_adhered_encoding_style': np.mean  # was np.mean\n",
    "    }\n",
    "    for col, fn in d_col_to_transform.items():\n",
    "        if col not in df_data:\n",
    "            print(col)\n",
    "            print(df_data.head())\n",
    "            print(example)\n",
    "            raise Exception(str(col) + \"\\n\" + str(df_data.head()) + \"\\n\" + str(example))\n",
    "\n",
    "        compute_ci_cols(example, df_data, col, fn)\n",
    "\n",
    "    df_data[\"num_tokens_translation_output\"] = ray.get(compute_translation_token_count.remote(example, df_data))\n",
    "\n",
    "    d_and_cols = {\n",
    "        'adherent_and_correct': (\n",
    "            lambda r: np.nanmean( \\\n",
    "                np.array(r['generated_cot_is_correct']).astype(bool) & \\\n",
    "                np.array(r['generated_cot_adhered_encoding_style']).astype(bool) \\\n",
    "            ),\n",
    "            np.nanmean\n",
    "        ),\n",
    "        'total_translation_loss': (\n",
    "            lambda r: np.nanmean( \\\n",
    "                np.array(r['num_tokens_translation_output']) * \\\n",
    "                np.array(np.nanmean(_score_rollouts(r['backtranslation_gt_logprobs'])), dtype=np.float64) \\\n",
    "            ),\n",
    "            np.nanmean\n",
    "        ),\n",
    "    }\n",
    "    for col, (transform_fn, agg_fn) in d_and_cols.items():\n",
    "        compute_multi_row_transformation(df_data, transform_fn, col, agg_fn)\n",
    "        if df_data[col].isna().sum() != len(df_data):\n",
    "            compute_ci_cols(example, df_data, col, lambda x: x)\n",
    "\n",
    "    if \"gpt\" in example[\"data\"][\"experiment_params\"][\"model\"] and \"use_api_sft_model_for_sampling\" in example[\"data\"][\"experiment_params\"]:\n",
    "        with open(os.path.join(root_dir, example['experiment_hash'], \"data\", f\"validation_reverse_translation_math500_meta.json\"), \"r\") as fp:\n",
    "            d_model_meta = json.load(fp)\n",
    "\n",
    "        example[\"backtranslation_gt_logprobs\"] = d_model_meta[\"valid_loss\"]\n",
    "        example[\"total_translation_loss\"] = d_model_meta[\"valid_loss\"] * np.nanmean(df_data[\"num_tokens_translation_output\"])\n",
    "        example[\"total_translation_loss_low_ci\"] = 0.0\n",
    "        example[\"total_translation_loss_hi_ci\"] = 0.0\n",
    "\n",
    "    pretraining_prevalence_path = os.path.join('/home/ubuntu/sky_workdir/encoding-schemes', 'output', experiment_hash, 'data', 'num_pretraining_4grams_redpajama.json')\n",
    "    if os.path.exists(pretraining_prevalence_path):\n",
    "        with open(pretraining_prevalence_path, \"r\") as fp:\n",
    "            d_pretraining_prevalence = json.load(fp)\n",
    "\n",
    "        example[\"pretraining_prevalence\"] = d_pretraining_prevalence[\"num_occurrences\"]\n",
    "\n",
    "\n",
    "    for temp in [0.001, 0.1, 0.3, 0.5, 0.7, 1.0]:\n",
    "        df_temp = f\"/home/ubuntu/sky_workdir/encoding-schemes/output/{experiment_hash}/data/temperature_{temp}/math_scores.parquet\"\n",
    "        if os.path.exists(df_temp):\n",
    "            df_temp = pd.read_parquet(df_temp)\n",
    "\n",
    "            if temp == 0.001:\n",
    "                temp = 0.0\n",
    "            example[f\"temp_{temp}_accuracy\"] = np.mean(list(df_temp['is_corrects']))\n",
    "\n",
    "    for col in df_data.columns:\n",
    "        example[f\"{col}_df\"] = df_data[col]\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "l_new_examples = [None for _ in range(len(l_examples))]\n",
    "\n",
    "for i, example in tqdm(enumerate(l_examples)):\n",
    "    # l_examples[i] = process_single_example(example)\n",
    "    l_new_examples[i] = process_single_example.remote(example)\n",
    "\n",
    "for i, example in tqdm(enumerate(l_new_examples)):\n",
    "    try:\n",
    "        l_new_examples[i] = ray.get(example)\n",
    "    except ray.exceptions.RayTaskError as e:\n",
    "        l_new_examples[i] = l_examples[i]\n",
    "        print(e)\n",
    "\n",
    "l_examples = l_new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7b0cc-4d72-4780-9f3a-0fa25ee3a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanize_number(num: float) -> str:\n",
    "    \"\"\"\n",
    "    Converts a number into a human-readable string with k, M, or B suffixes.\n",
    "    \n",
    "    Args:\n",
    "        num (float): The number to format.\n",
    "    \n",
    "    Returns:\n",
    "        str: Human-readable string representation.\n",
    "    \"\"\"\n",
    "    if num >= 1_000_000_000:\n",
    "        return f\"{num / 1_000_000_000:.1f}B\"\n",
    "    elif num >= 1_000_000:\n",
    "        return f\"{num / 1_000_000:.1f}M\"\n",
    "    elif num >= 1_000:\n",
    "        return f\"{num / 1_000:.1f}k\"\n",
    "    else:\n",
    "        return str(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c8aa3-b7b1-48a7-b0fc-c8811fe9a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_params(model):\n",
    "    if 'gpt' in model:\n",
    "        if 'nano' in model:\n",
    "            return 0\n",
    "        elif 'mini' in model:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "\n",
    "    if 'claude' in model:\n",
    "        if 'haiku' in model:\n",
    "            return 3\n",
    "        elif 'sonnet' in model:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "    \n",
    "    return int(re.search(\"([0-9]+)B\", model).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a412a1f-de17-4d22-b798-12ea8ce0db67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_viz = pd.DataFrame(l_examples)\n",
    "\n",
    "orig_len = len(df_viz)\n",
    "\n",
    "# df_viz = df_viz[df_viz['cot_gt_logprobs'].notna()]\n",
    "\n",
    "new_len = len(df_viz)\n",
    "if orig_len != new_len:\n",
    "    print(f\"Lost {orig_len - new_len} examples from na logprobs\")\n",
    "\n",
    "df_viz['encoding_scheme'] = df_viz['data'].map(lambda x: x['experiment_params']['encoding_scheme'])\n",
    "df_viz['model'] = df_viz['data'].map(lambda x: x['experiment_params']['model'])\n",
    "\n",
    "try:\n",
    "    df_viz['model_size'] = df_viz['model'].map(parse_params)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "df_viz['input_type'] = df_viz['data'].map(lambda x: \"_\".join(x['experiment_name'].split(\"_\")[:2]))\n",
    "\n",
    "df_viz['n_few_shot_examples'] = df_viz['data'].map(lambda x: x['experiment_params'].get('n_few_shot_examples', None))\n",
    "\n",
    "df_viz['Adherence Calculation Method'] = df_viz['encoding_scheme'].map(lambda x: get_deterministic_adherence_fn(x, None) is not None).map({ True: 'deterministic', False: 'Sonnet 4 judge'})\n",
    "\n",
    "try:\n",
    "    df_viz['total_train_tok'] = df_viz['n_total_train_tok'].map(humanize_number)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "df_viz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59da245-a8a4-482d-b5a8-814c663f4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz['input_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d5f60-3ce3-448f-b887-0d0723e8a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_set = ['mathcot_fewshot']\n",
    "filter_set = ['math_cot']\n",
    "# filter_set = ['math_cot', 'mathcot_fewshot']\n",
    "# filter_set = ['mathcot_prompted']\n",
    "# filter_set = ['mathcot_prompteddecode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294af9ce-482f-4336-a8bc-6aaf90027383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp = df_viz[df_viz['input_type'].isin(filter_set)]\n",
    "df_viz_tmp = df_viz_tmp[df_viz_tmp['model'] != 'Qwen/Qwen2.5-7B']\n",
    "\n",
    "df_viz_tmp = df_viz_tmp.sort_values([\n",
    "    'model_size',\n",
    "    'adherent_and_correct'\n",
    "])\n",
    "\n",
    "df_viz_tmp = df_viz_tmp.astype({'n_few_shot_examples': str})\n",
    "\n",
    "df_viz_tmp['encoding_scheme'] = df_viz_tmp['encoding_scheme'].map(lambda s: s.split(\"speaking_\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe71a8ce-22e0-4bb2-9c4f-8f6661caee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp[df_viz_tmp['model'].str.contains('gpt')]['encoding_scheme'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036194e-f866-4aaa-a7e0-dcf1426c654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Melt the dataframe\n",
    "df_plot = pd.melt(df_viz_tmp, \n",
    "                  id_vars=['encoding_scheme', 'model'], \n",
    "                  value_vars=[f\"temp_{temp}_accuracy\" for temp in [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]])\n",
    "\n",
    "df_plot = df_plot[df_plot['value'].notna()]\n",
    "df_plot = df_plot[df_plot['model'].str.contains('14B')]\n",
    "\n",
    "# Create a color map for the temperature values\n",
    "temps = [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "# Create colors from a continuous colormap\n",
    "import plotly.colors as pc\n",
    "colors = pc.sample_colorscale('Plasma', np.linspace(0, 1, len(temps)))\n",
    "\n",
    "# Map variable names to colors\n",
    "color_discrete_map = {f\"temp_{temp}_accuracy\": colors[i] for i, temp in enumerate(temps)}\n",
    "\n",
    "# Create the bar chart\n",
    "fig = px.bar(df_plot, \n",
    "             x='encoding_scheme', \n",
    "             color='variable', \n",
    "             y='value', \n",
    "             barmode='group',\n",
    "             color_discrete_map=color_discrete_map,\n",
    "             labels={'value': 'Accuracy relative to identity', \n",
    "                     'variable': 'Temperature',\n",
    "                     'encoding_scheme': 'Encoding Scheme'})\n",
    "\n",
    "# Update legend labels to show just temperature values\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace('temp_', 'T=').replace('_accuracy', '')))\n",
    "\n",
    "fig.update_layout(title='Accuracy Relative to Identity by Encoding Scheme and Temperature, Qwen2.5 14B')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110280fe-99d4-4a97-8cdc-9edb000dff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "d_mapping = {}\n",
    "for _, row in df_viz_tmp.iterrows():\n",
    "    for temp in [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]:\n",
    "        d_mapping[(row['model'], row['encoding_scheme'], temp)] = df_viz_tmp[(df_viz_tmp['model'] == row['model']) & (df_viz_tmp['encoding_scheme'] == 'identity')][f\"temp_{temp}_accuracy\"].iloc[0]\n",
    "\n",
    "for temp in [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]:\n",
    "    df_viz_tmp[f\"temp_{temp}_rel_acc\"] = df_viz_tmp.apply(lambda r: r[f\"temp_{temp}_accuracy\"] / d_mapping[(r['model'], r['encoding_scheme'], temp)], axis=1)\n",
    "\n",
    "# Melt the dataframe\n",
    "df_plot = pd.melt(df_viz_tmp, \n",
    "                  id_vars=['encoding_scheme', 'model'], \n",
    "                  value_vars=[f\"temp_{temp}_rel_acc\" for temp in [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]])\n",
    "\n",
    "df_plot = df_plot[df_plot['value'].notna()]\n",
    "df_plot = df_plot[df_plot['model'].str.contains('14B')]\n",
    "\n",
    "# Create a color map for the temperature values\n",
    "temps = [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "# Create colors from a continuous colormap\n",
    "import plotly.colors as pc\n",
    "colors = pc.sample_colorscale('Plasma', np.linspace(0, 1, len(temps)))\n",
    "\n",
    "# Map variable names to colors\n",
    "color_discrete_map = {f\"temp_{temp}_rel_acc\": colors[i] for i, temp in enumerate(temps)}\n",
    "\n",
    "# Create the bar chart\n",
    "fig = px.bar(df_plot, \n",
    "             x='encoding_scheme', \n",
    "             color='variable', \n",
    "             y='value', \n",
    "             barmode='group',\n",
    "             color_discrete_map=color_discrete_map,\n",
    "             labels={'value': 'Accuracy relative to identity', \n",
    "                     'variable': 'Temperature',\n",
    "                     'encoding_scheme': 'Encoding Scheme'})\n",
    "\n",
    "# Update legend labels to show just temperature values\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace('temp_', 'T=').replace('_rel_acc', '')))\n",
    "\n",
    "fig.update_layout(title='Accuracy Relative to Identity by Encoding Scheme and Temperature, Qwen2.5 14B')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9584995-2c80-48a0-a799-074bc4d0487e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22808116-bd64-48fe-be10-423815877727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513343d-0c64-4054-a2f7-291dbe1d2378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5421f4b-57ca-47d0-af1f-3f42950ab1d4",
   "metadata": {},
   "source": [
    "# Adherence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559e93b-ed7e-473a-b1e1-e40b2d723072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "\n",
    "fig = px.bar(df_viz_tmp_plot, x='encoding_scheme', y='generated_cot_adhered_encoding_style',\n",
    "             height=800, width=1600,\n",
    "             # height=1600, width=1600,\n",
    "             # color='model',\n",
    "             error_y='generated_cot_adhered_encoding_style_hi_ci',\n",
    "             error_y_minus='generated_cot_adhered_encoding_style_low_ci',\n",
    "             # color='n_few_shot_examples',\n",
    "             # facet_row='model',\n",
    "             color='model',\n",
    "             facet_col='Adherence Calculation Method',\n",
    "             # color='training_augmentation',\n",
    "             # color='total_train_tok',\n",
    "             # color_discrete_map=color_discrete_map,\n",
    "             title=\"MATH 500 CoT encoding style adherence\",\n",
    "             barmode=\"group\",\n",
    "             template=\"plotly_white\",\n",
    "             color_discrete_sequence=px.colors.qualitative.Set2,\n",
    "            )\n",
    "\n",
    "fig.update_xaxes(title=\"Encoding scheme\", tickangle=90)\n",
    "fig.update_yaxes(title=\"% adherent encodings\", dtick=0.1)\n",
    "\n",
    "# ✅ Make each facet's x-axis independent to avoid empty slots\n",
    "fig.for_each_xaxis(lambda ax: ax.update(matches=None, categoryorder='trace'))\n",
    "\n",
    "# ✅ Align all x-axis titles by setting a fixed vertical offset\n",
    "ct = [0]\n",
    "def ax_standoff_updater(ax, ct):    \n",
    "    ax.title.update(standoff=ct[0] * 130)\n",
    "    ct[0] += 1\n",
    "\n",
    "fig.for_each_xaxis(lambda ax: ax_standoff_updater(ax, ct))\n",
    "fig.update_yaxes(title_standoff=5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c9963-fa36-472f-ae78-e4eba8dacc45",
   "metadata": {},
   "source": [
    "# Pretraining prevalence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e7574-2e86-424d-a668-159295cedcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "df_viz_tmp_plot['encoding_scheme'] = df_viz_tmp_plot['encoding_scheme'].str.replace('_', ' ')\n",
    "\n",
    "l_algo_encodings = [\n",
    "    'base64 cipher',\n",
    "    'base64 2x cipher',\n",
    "    'base64 3x cipher',\n",
    "    'gzip to base64 encoded',\n",
    "    'letter to word with dot',\n",
    "    'dot between chars',\n",
    "    'space between chars'\n",
    "]\n",
    "\n",
    "# Separate data into two groups\n",
    "df_l_algo = df_viz_tmp_plot[df_viz_tmp_plot['encoding_scheme'].isin(l_algo_encodings)]\n",
    "df_rest = df_viz_tmp_plot[~df_viz_tmp_plot['encoding_scheme'].isin(l_algo_encodings)]\n",
    "\n",
    "# Create the scatter plot with color coding\n",
    "df_viz_tmp_plot['group'] = df_viz_tmp_plot['encoding_scheme'].apply(\n",
    "    lambda x: 'Algorithmic' if x in l_algo_encodings else 'Language'\n",
    ")\n",
    "\n",
    "fig = px.scatter(df_viz_tmp_plot, x='pretraining_prevalence', y='adherent_and_correct', \n",
    "                 log_x=True, text='encoding_scheme', color='group',\n",
    "                 color_discrete_map={'Algorithmic': 'red', 'Language': 'blue'})\n",
    "\n",
    "# Update the scatter trace to not be part of the legend and keep its original appearance\n",
    "fig.update_traces(textposition=\"top center\", showlegend=False, selector=dict(mode='markers+text'))\n",
    "fig.update_layout(legend_title_text=None) \n",
    "\n",
    "# Define fit functions\n",
    "def linear_func(x, a, b):\n",
    "    \"\"\"Linear function: y = ax + b\"\"\"\n",
    "    return a * x + b\n",
    "\n",
    "def exponential_func(x, a, b, c):\n",
    "    \"\"\"Exponential function: y = a * exp(b * x) + c\"\"\"\n",
    "    return a * np.exp(b * x) + c\n",
    "\n",
    "def sigmoid_func(x, a, b, c, d):\n",
    "    \"\"\"Sigmoid function: y = a / (1 + exp(-b * (x - c))) + d\"\"\"\n",
    "    return a / (1 + np.exp(-b * (x - c))) + d\n",
    "\n",
    "def calculate_fit(df, fit_type='linear', use_log_x=True):\n",
    "    \"\"\"\n",
    "    Calculate trend line based on specified fit type.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with data\n",
    "    - fit_type: 'linear', 'exponential', or 'sigmoid'\n",
    "    - use_log_x: Whether to use log-transformed x values\n",
    "    \"\"\"\n",
    "    # Remove any rows with NaN or zero/negative x values\n",
    "    df_clean = df.dropna(subset=['pretraining_prevalence', 'generated_cot_is_correct'])\n",
    "    df_clean = df_clean[df_clean['pretraining_prevalence'] > 0]\n",
    "    \n",
    "    if len(df_clean) < 2:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Prepare x and y values\n",
    "    if use_log_x:\n",
    "        x = np.log10(df_clean['pretraining_prevalence'].values)\n",
    "    else:\n",
    "        x = df_clean['pretraining_prevalence'].values\n",
    "    y = df_clean['adherent_and_correct'].values\n",
    "    \n",
    "    try:\n",
    "        if fit_type == 'linear':\n",
    "            # Linear regression\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            params = [slope, intercept]\n",
    "            \n",
    "            # Calculate predictions for R²\n",
    "            y_pred = linear_func(x, *params)\n",
    "            \n",
    "        elif fit_type == 'exponential':\n",
    "            # Exponential fit\n",
    "            # Initial guess: [amplitude, growth_rate, offset]\n",
    "            initial_guess = [0.5, 0.1, 0.3]\n",
    "            bounds = ([0, -np.inf, 0], [1, np.inf, 1])\n",
    "            params, _ = curve_fit(exponential_func, x, y, p0=initial_guess, \n",
    "                                bounds=bounds, maxfev=5000)\n",
    "            \n",
    "            # Calculate predictions for R²\n",
    "            y_pred = exponential_func(x, *params)\n",
    "            \n",
    "        elif fit_type == 'sigmoid':\n",
    "            # Sigmoid fit\n",
    "            # Initial guess: [amplitude, steepness, x_mid, y_offset]\n",
    "            x_mid = np.median(x)\n",
    "            initial_guess = [0.7, 1.0, x_mid, 0.2]\n",
    "            bounds = ([0, 0, x.min(), 0], [1, 10, x.max(), 1])\n",
    "            params, _ = curve_fit(sigmoid_func, x, y, p0=initial_guess, \n",
    "                                bounds=bounds, maxfev=5000)\n",
    "            \n",
    "            # Calculate predictions for R²\n",
    "            y_pred = sigmoid_func(x, *params)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fit_type: {fit_type}\")\n",
    "        \n",
    "        # Calculate R²\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "        \n",
    "        return params, r_squared, fit_type\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting {fit_type}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Configuration - CHANGE THESE TO SELECT FIT TYPE\n",
    "FIT_TYPE = 'linear'  # Options: 'linear', 'exponential', 'sigmoid'\n",
    "USE_LOG_X = False  # Whether to use log-transformed x for fitting\n",
    "\n",
    "# Calculate trends for both groups\n",
    "params_l_algo, r2_l_algo, _ = calculate_fit(df_l_algo, fit_type=FIT_TYPE, use_log_x=USE_LOG_X)\n",
    "params_rest, r2_rest, _ = calculate_fit(df_rest, fit_type=FIT_TYPE, use_log_x=USE_LOG_X)\n",
    "\n",
    "# Generate trend line points\n",
    "x_range = df_viz_tmp_plot['pretraining_prevalence'].values\n",
    "x_range = x_range[x_range > 0]  # Remove non-positive values\n",
    "x_range = np.sort(x_range)\n",
    "\n",
    "if USE_LOG_X:\n",
    "    x_fit_range = np.log10(x_range)\n",
    "else:\n",
    "    x_fit_range = x_range\n",
    "\n",
    "# Add trend lines if calculations were successful\n",
    "if params_l_algo is not None:\n",
    "    if FIT_TYPE == 'linear':\n",
    "        y_trend_l_algo = linear_func(x_fit_range, *params_l_algo)\n",
    "    elif FIT_TYPE == 'exponential':\n",
    "        y_trend_l_algo = exponential_func(x_fit_range, *params_l_algo)\n",
    "    elif FIT_TYPE == 'sigmoid':\n",
    "        y_trend_l_algo = sigmoid_func(x_fit_range, *params_l_algo)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_range,\n",
    "        y=y_trend_l_algo,\n",
    "        mode='lines',\n",
    "        name=f'Algorithmic encoding, {FIT_TYPE} fit (R²={r2_l_algo:.3f})',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "if params_rest is not None:\n",
    "    if FIT_TYPE == 'linear':\n",
    "        y_trend_rest = linear_func(x_fit_range, *params_rest)\n",
    "    elif FIT_TYPE == 'exponential':\n",
    "        y_trend_rest = exponential_func(x_fit_range, *params_rest)\n",
    "    elif FIT_TYPE == 'sigmoid':\n",
    "        y_trend_rest = sigmoid_func(x_fit_range, *params_rest)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_range,\n",
    "        y=y_trend_rest,\n",
    "        mode='lines',\n",
    "        name=f'Language encoding, {FIT_TYPE} fit (R²={r2_rest:.3f})',\n",
    "        line=dict(color='blue', dash='dash'),\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    width=800,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ),\n",
    "    # title=f\"MATH500 Accuracy vs Pretraining Prevalence ({FIT_TYPE.capitalize()} Fit)\"\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=40, b=20))\n",
    "\n",
    "fig.update_yaxes(dtick=0.1, title=\"% adherent & correct responses\", range=[0, 0.6])\n",
    "fig.update_xaxes(title=\"Pretraining prevalence\", type=\"log\")\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "fig.update_traces(textfont_size=10)\n",
    "\n",
    "fig.show('png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636c594-1907-4a0d-bd0a-62aef13b926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "df_viz_tmp_plot['encoding_scheme'] = df_viz_tmp_plot['encoding_scheme'].str.replace('_', ' ')\n",
    "\n",
    "l_algo_encodings = [\n",
    "    'base64 cipher',\n",
    "    'base64 2x cipher',\n",
    "    'base64 3x cipher',\n",
    "    'gzip to base64 encoded',\n",
    "    'letter to word with dot',\n",
    "    'dot between chars',\n",
    "    'space between chars'\n",
    "]\n",
    "\n",
    "# Separate data into two groups\n",
    "df_l_algo = df_viz_tmp_plot[df_viz_tmp_plot['encoding_scheme'].isin(l_algo_encodings)]\n",
    "df_rest = df_viz_tmp_plot[~df_viz_tmp_plot['encoding_scheme'].isin(l_algo_encodings)]\n",
    "\n",
    "# Create the scatter plot with color coding\n",
    "df_viz_tmp_plot['group'] = df_viz_tmp_plot['encoding_scheme'].apply(\n",
    "    lambda x: 'Algorithmic' if x in l_algo_encodings else 'Language'\n",
    ")\n",
    "\n",
    "fig = px.scatter(df_viz_tmp_plot, x='pretraining_prevalence', y='generated_cot_is_correct', \n",
    "                 log_x=True, text='encoding_scheme', color='group',\n",
    "                 color_discrete_map={'Algorithmic': 'red', 'Language': 'blue'})\n",
    "\n",
    "# Update the scatter trace to not be part of the legend and keep its original appearance\n",
    "fig.update_traces(textposition=\"top center\", showlegend=False, selector=dict(mode='markers+text'))\n",
    "fig.update_layout(legend_title_text=None) \n",
    "\n",
    "# Define fit functions\n",
    "def linear_func(x, a, b):\n",
    "    \"\"\"Linear function: y = ax + b\"\"\"\n",
    "    return a * x + b\n",
    "\n",
    "def exponential_func(x, a, b, c):\n",
    "    \"\"\"Exponential function: y = a * exp(b * x) + c\"\"\"\n",
    "    return a * np.exp(b * x) + c\n",
    "\n",
    "def sigmoid_func(x, a, b, c, d):\n",
    "    \"\"\"Sigmoid function: y = a / (1 + exp(-b * (x - c))) + d\"\"\"\n",
    "    return a / (1 + np.exp(-b * (x - c))) + d\n",
    "\n",
    "def calculate_fit(df, fit_type='linear', use_log_x=True):\n",
    "    \"\"\"\n",
    "    Calculate trend line based on specified fit type.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with data\n",
    "    - fit_type: 'linear', 'exponential', or 'sigmoid'\n",
    "    - use_log_x: Whether to use log-transformed x values\n",
    "    \"\"\"\n",
    "    # Remove any rows with NaN or zero/negative x values\n",
    "    df_clean = df.dropna(subset=['pretraining_prevalence', 'generated_cot_is_correct'])\n",
    "    df_clean = df_clean[df_clean['pretraining_prevalence'] > 0]\n",
    "    \n",
    "    if len(df_clean) < 2:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Prepare x and y values\n",
    "    if use_log_x:\n",
    "        x = np.log10(df_clean['pretraining_prevalence'].values)\n",
    "    else:\n",
    "        x = df_clean['pretraining_prevalence'].values\n",
    "    y = df_clean['generated_cot_is_correct'].values\n",
    "    \n",
    "    try:\n",
    "        if fit_type == 'linear':\n",
    "            # Linear regression\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            params = [slope, intercept]\n",
    "            \n",
    "            # Calculate predictions for R²\n",
    "            y_pred = linear_func(x, *params)\n",
    "            \n",
    "        elif fit_type == 'exponential':\n",
    "            # Exponential fit\n",
    "            # Initial guess: [amplitude, growth_rate, offset]\n",
    "            initial_guess = [0.5, 0.1, 0.3]\n",
    "            bounds = ([0, -np.inf, 0], [1, np.inf, 1])\n",
    "            params, _ = curve_fit(exponential_func, x, y, p0=initial_guess, \n",
    "                                bounds=bounds, maxfev=5000)\n",
    "            \n",
    "            # Calculate predictions for R²\n",
    "            y_pred = exponential_func(x, *params)\n",
    "            \n",
    "        elif fit_type == 'sigmoid':\n",
    "            # Sigmoid fit\n",
    "            # Initial guess: [amplitude, steepness, x_mid, y_offset]\n",
    "            x_mid = np.median(x)\n",
    "            initial_guess = [0.7, 1.0, x_mid, 0.2]\n",
    "            bounds = ([0, 0, x.min(), 0], [1, 10, x.max(), 1])\n",
    "            params, _ = curve_fit(sigmoid_func, x, y, p0=initial_guess, \n",
    "                                bounds=bounds, maxfev=5000)\n",
    "            \n",
    "            # Calculate predictions for R²\n",
    "            y_pred = sigmoid_func(x, *params)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fit_type: {fit_type}\")\n",
    "        \n",
    "        # Calculate R²\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "        \n",
    "        return params, r_squared, fit_type\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting {fit_type}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Configuration - CHANGE THESE TO SELECT FIT TYPE\n",
    "FIT_TYPE = 'linear'  # Options: 'linear', 'exponential', 'sigmoid'\n",
    "USE_LOG_X = False  # Whether to use log-transformed x for fitting\n",
    "\n",
    "# Calculate trends for both groups\n",
    "params_l_algo, r2_l_algo, _ = calculate_fit(df_l_algo, fit_type=FIT_TYPE, use_log_x=USE_LOG_X)\n",
    "params_rest, r2_rest, _ = calculate_fit(df_rest, fit_type=FIT_TYPE, use_log_x=USE_LOG_X)\n",
    "\n",
    "# Generate trend line points\n",
    "x_range = df_viz_tmp_plot['pretraining_prevalence'].values\n",
    "x_range = x_range[x_range > 0]  # Remove non-positive values\n",
    "x_range = np.sort(x_range)\n",
    "\n",
    "if USE_LOG_X:\n",
    "    x_fit_range = np.log10(x_range)\n",
    "else:\n",
    "    x_fit_range = x_range\n",
    "\n",
    "# Add trend lines if calculations were successful\n",
    "if params_l_algo is not None:\n",
    "    if FIT_TYPE == 'linear':\n",
    "        y_trend_l_algo = linear_func(x_fit_range, *params_l_algo)\n",
    "    elif FIT_TYPE == 'exponential':\n",
    "        y_trend_l_algo = exponential_func(x_fit_range, *params_l_algo)\n",
    "    elif FIT_TYPE == 'sigmoid':\n",
    "        y_trend_l_algo = sigmoid_func(x_fit_range, *params_l_algo)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_range,\n",
    "        y=y_trend_l_algo,\n",
    "        mode='lines',\n",
    "        name=f'Algorithmic encoding, {FIT_TYPE} fit (R²={r2_l_algo:.3f})',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "if params_rest is not None:\n",
    "    if FIT_TYPE == 'linear':\n",
    "        y_trend_rest = linear_func(x_fit_range, *params_rest)\n",
    "    elif FIT_TYPE == 'exponential':\n",
    "        y_trend_rest = exponential_func(x_fit_range, *params_rest)\n",
    "    elif FIT_TYPE == 'sigmoid':\n",
    "        y_trend_rest = sigmoid_func(x_fit_range, *params_rest)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_range,\n",
    "        y=y_trend_rest,\n",
    "        mode='lines',\n",
    "        name=f'Language encoding, {FIT_TYPE} fit (R²={r2_rest:.3f})',\n",
    "        line=dict(color='blue', dash='dash'),\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    width=800,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    ),\n",
    "    # title=f\"MATH500 Accuracy vs Pretraining Prevalence ({FIT_TYPE.capitalize()} Fit)\"\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=40, b=20))\n",
    "\n",
    "fig.update_yaxes(dtick=0.1, title=\"MATH500 accuracy\", range=[0, 0.6])\n",
    "fig.update_xaxes(title=\"Pretraining prevalence\", type=\"log\")\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "fig.update_traces(textfont_size=10)\n",
    "\n",
    "fig.show('png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a828d6-6815-4381-b995-20982640f527",
   "metadata": {},
   "source": [
    "# Paper plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea4d31-60d6-443c-8d80-a06febf33739",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mapping = {\n",
    "    \"baseline\": [\"zero_shot\", \"identity\"],\n",
    "    \"letter mutation\": [\n",
    "        \"reverse_letters_in_each_word\",\n",
    "        \"swap_even_odd_letters_in_each_word\",\n",
    "        \"reverse_fibonacci_indices_in_each_word\",\n",
    "        \"letter_to_word_with_dot\",\n",
    "        \"dot_between_chars\",\n",
    "        \"space_between_chars\",\n",
    "    ],\n",
    "    \"language deletion\": [\"remove_all_verbs\", \"remove_all_nouns\"],\n",
    "    \"language translation\": [\n",
    "        \"French\",\"Chinese\",\"Korean\",\"Russian\",\"Arabic\",\"Adyghe\",\n",
    "        \"Morse_code\",\"Python\",\"enterprise_Java\",\n",
    "    ],\n",
    "    \"algorithmic cipher\": [\n",
    "        \"rot13_cipher\",\"base64_cipher\",\"base64_2x_cipher\",\n",
    "        \"base64_3x_cipher\",\"caesar_cipher\",\"gzip_to_base64_encoded\",\n",
    "    ],\n",
    "    \"themed reasoning\": [\n",
    "        \"paraphrase_naive\",\n",
    "        \"pirate_speak\",\n",
    "        \"leet_speak\",\n",
    "        \"yoda_speak\",\n",
    "        \"shakespearean_text\",\n",
    "    ],\n",
    "    \"extraneous content\": [\n",
    "        \"insert_tweet\",\n",
    "        \"python_snippet_comment\",\n",
    "        \"croissant_news_article\",\n",
    "        \"math_textbook_article\",\n",
    "        \"five_emojis\",\n",
    "    ],\n",
    "    \"delete inf.\": [\n",
    "        \"replace_math_content_with_black_box\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "l_themed_encodings = [\n",
    "    \"paraphrase_naive\",\n",
    "    \"pirate_speak\",\n",
    "    \"leet_speak\",\n",
    "    \"yoda_speak\",\n",
    "    \"shakespearean_text\",\n",
    "    \"insert_tweet\",\n",
    "    \"python_snippet_comment\",\n",
    "    \"croissant_news_article\",\n",
    "    \"math_textbook_article\",\n",
    "    \"five_emojis\",\n",
    "    \"replace_math_content_with_black_box\",\n",
    "    \"reverse_letters_in_each_word_no_math_expressions\",\n",
    "    \"reverse_letters_in_each_word_only_math_expressions\"\n",
    "]\n",
    "\n",
    "l_ignore_languages = [\n",
    "    \"Russian\",\n",
    "    \"Chinese\",\n",
    "    # \"Korean\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b0308-1806-4005-80ec-d83ab4716125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.colors as pc\n",
    "\n",
    "def sample_colorscale(colorscale_name: str, n: int):\n",
    "    \"\"\"\n",
    "    Sample N equally spaced hex colors from a Plotly continuous colorscale.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    colorscale_name : str\n",
    "        Name of a Plotly built-in continuous colorscale (e.g., \"Viridis\", \"Blues\").\n",
    "    n : int\n",
    "        Number of samples to return.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        List of hex color strings.\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be >= 1\")\n",
    "    colorscale = px.colors.get_colorscale(colorscale_name)\n",
    "    # equally spaced points in [0,1]\n",
    "    vals = [i/(n-1) if n > 1 else 0.5 for i in range(n)]\n",
    "    return [pc.sample_colorscale(colorscale, v)[0] for v in vals]\n",
    "\n",
    "l_gpt_gradient = sample_colorscale(\"Emrld\", 4 + 1)\n",
    "d_gpt_gradient = {\n",
    "    model : color\n",
    "    for model, color in zip(\n",
    "        ['gpt-4.1-nano-2025-04-14', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-2025-04-14', 'gpt-5-chat-latest'],\n",
    "        l_gpt_gradient[1:]\n",
    "    )\n",
    "}\n",
    "\n",
    "l_claude_gradient = sample_colorscale(\"Magenta\", 3 + 1)\n",
    "d_claude_gradient = {\n",
    "    model : color\n",
    "    for model, color in zip(\n",
    "        ['claude-3-opus-20240229', 'claude-3-5-sonnet-20241022', 'claude-sonnet-4-20250514'],\n",
    "        l_claude_gradient[1:]\n",
    "    )\n",
    "}\n",
    "\n",
    "l_qwen_gradient = sample_colorscale(\"Oryel\", 3 + 1)\n",
    "d_qwen_gradient = {\n",
    "    model : color\n",
    "    for model, color in zip(\n",
    "        ['Qwen2.5-3B-Instruct', 'Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct'],\n",
    "        l_qwen_gradient[1:]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f323df5-d4f6-4e81-88e7-7ca6b53f61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp[df_viz_tmp['model'].str.contains('gpt')]['encoding_scheme'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbea54-0ca9-49b4-a54c-12699bd2d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_star_encodings = [\n",
    "    \"reverse_letters_in_each_word\",\n",
    "    \"rot13_cipher\",\n",
    "    \"caesar_cipher\", # TODO check\n",
    " 'reverse_fibonacci_indices_in_each_word', # TODO check\n",
    " 'swap_even_odd_letters_in_each_word', # TODO check\n",
    "    \"Morse_code\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2ffbe-0a88-412a-84ba-ef78415e4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_fixed_encoding_ordering = [\n",
    "    # GPT\n",
    "    'identity',\n",
    "    'dot_between_chars',\n",
    "    'Korean',\n",
    "    'letter_to_word_with_dot',\n",
    "    'rot13_cipher',\n",
    "    'remove_all_verbs',\n",
    "    'Morse_code',\n",
    "    'base64_cipher',\n",
    "    'reverse_letters_in_each_word',\n",
    "\n",
    "    # placeholder for an empty space\n",
    "    '',\n",
    "\n",
    "    # Qwen/Claude\n",
    " 'French',\n",
    " 'space_between_chars',\n",
    " 'Arabic',\n",
    " 'Adyghe',\n",
    " 'caesar_cipher',\n",
    " 'swap_even_odd_letters_in_each_word',\n",
    "    'reverse_fibonacci_indices_in_each_word',\n",
    "    'remove_all_nouns',\n",
    " 'Python',\n",
    "    'enterprise_Java',\n",
    "'base64_2x_cipher',\n",
    " 'base64_3x_cipher',\n",
    " 'gzip_to_base64_encoded',\n",
    " \n",
    "]\n",
    "\n",
    "# for i in range(len(l_fixed_encoding_ordering)):\n",
    "#     if l_fixed_encoding_ordering[i] in l_star_encodings:\n",
    "#         l_fixed_encoding_ordering[i] += \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c55d5-a543-4d29-8438-7f8edd662956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute PGR (percent of identity performance) ---\n",
    "df_pgr = df_viz_tmp.copy()\n",
    "\n",
    "df_pgr = pd.concat([\n",
    "    df_pgr,\n",
    "    pd.DataFrame([{'encoding_scheme': '', 'model': model} for model in df_pgr['model'].unique()])\n",
    "], ignore_index=True)\n",
    "\n",
    "df_pgr['model'] = df_pgr['model'].str.split('Qwen/').str[-1]\n",
    "\n",
    "# Identity baseline per model\n",
    "identity_baseline = (\n",
    "    df_pgr[df_pgr[\"encoding_scheme\"] == \"identity\"]\n",
    "    .set_index(\"model\")[\"adherent_and_correct\"]\n",
    ")\n",
    "\n",
    "df_pgr[\"identity_value\"] = np.maximum(df_pgr[\"model\"].map(identity_baseline), 0.0001)\n",
    "\n",
    "# Avoid divide-by-zero\n",
    "# df_pgr = df_pgr[df_pgr[\"identity_value\"] > 0].copy()\n",
    "\n",
    "# Mean PGR as percentage\n",
    "df_pgr[\"PGR_pct\"] = (df_pgr[\"adherent_and_correct\"] / df_pgr[\"identity_value\"])\n",
    "\n",
    "# CI deltas -> percentage deltas relative to identity baseline\n",
    "# low is negative, high is positive\n",
    "df_pgr[\"PGR_pct_hi_ci\"]  = (df_pgr[\"adherent_and_correct_hi_ci\"] / df_pgr[\"identity_value\"])\n",
    "df_pgr[\"PGR_pct_low_ci\"] = (df_pgr[\"adherent_and_correct_low_ci\"]        / df_pgr[\"identity_value\"])\n",
    "\n",
    "df_pgr.loc[df_pgr[\"encoding_scheme\"] == \"identity\", [\"PGR_pct_hi_ci\", \"PGR_pct_low_ci\"]] = 0.0\n",
    "\n",
    "# order by sonnet 4 hard coded\n",
    "# df_sonnet4 = df_pgr[df_pgr['model'].str.contains('claude-3-5-sonnet')].sort_values('adherent_and_correct', ascending=False)\n",
    "# df_sonnet4 = df_pgr[df_pgr['model'].str.contains('14B')].sort_values('adherent_and_correct', ascending=False)\n",
    "# df_sonnet4 = df_pgr[df_pgr['model'].str.contains('sonnet-4')].sort_values('adherent_and_correct', ascending=False)\n",
    "\n",
    "# d_encoding_scheme_to_idx = {}\n",
    "# for i in range(len(df_sonnet4)):\n",
    "#     d_encoding_scheme_to_idx[df_sonnet4['encoding_scheme'].iloc[i]] = i\n",
    "\n",
    "# d_encoding_scheme_to_idx['identity'] = 0\n",
    "\n",
    "for i, encoding in enumerate(l_fixed_encoding_ordering):\n",
    "    d_encoding_scheme_to_idx[encoding] = i\n",
    "\n",
    "df_pgr = df_pgr[~df_pgr['encoding_scheme'].isin(l_themed_encodings)]\n",
    "df_pgr = df_pgr[~df_pgr['encoding_scheme'].isin(l_ignore_languages)]\n",
    "\n",
    "# df_pgr['encoding_scheme'] = df_pgr['encoding_scheme'].map(lambda x: x + '*' if x in l_star_encodings else x)\n",
    "\n",
    "df_pgr['sort_order'] = df_pgr['encoding_scheme'].map(d_encoding_scheme_to_idx)\n",
    "df_pgr = df_pgr.sort_values(['sort_order', 'model_size'])\n",
    "\n",
    "df_pgr['encoding_scheme'] = df_pgr['encoding_scheme'].map(lambda s: \" \".join(s.split('_')))\n",
    "\n",
    "df_pgr['Model Family'] = df_pgr[\"model\"].map(lambda x: \"Qwen2.5\" if \"Qwen2.5\" in x else \"GPT\" if \"gpt\" in x else \"Claude\")\n",
    "\n",
    "fig = px.line(df_pgr[df_pgr['encoding_scheme'] != 'zero shot'],\n",
    "              x='encoding_scheme',\n",
    "              y='PGR_pct',\n",
    "              color='model',\n",
    "              color_discrete_map={**d_claude_gradient, **d_gpt_gradient, **d_qwen_gradient},\n",
    "              markers=True,\n",
    "              # facet_col=\"Model Family\"\n",
    ")\n",
    "\n",
    "# s_encoding_scheme_order = df_pgr[df_pgr['model'].str.contains('14B') & (df_pgr['encoding_scheme'] != 'zero shot')]['encoding_scheme']\n",
    "# s_encoding_scheme_order = df_pgr[df_pgr['model'].str.contains('sonnet-4') & (df_pgr['encoding_scheme'] != 'zero shot')]['encoding_scheme']\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        categoryorder=\"array\",\n",
    "        # categoryarray=s_encoding_scheme_order\n",
    "        categoryarray=[' '.join(s.split('_')) for s in l_fixed_encoding_ordering]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=dict(text=title, font=dict(size=22)),\n",
    "    template=\"plotly_white\",\n",
    "    height=500,\n",
    "    width=1800,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.08,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01,\n",
    "        font=dict(size=10)\n",
    "    ),\n",
    "    margin=dict(t=110, r=30, b=80, l=70),\n",
    "    font=dict(size=13),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title=\"Encoding scheme\")\n",
    "fig.update_yaxes(range=[0.0, 1.05], dtick=0.1, title=\"% of identity adherent & correct\")\n",
    "\n",
    "# add the zero shot lines for 4.1 and 14b\n",
    "l_zero_shot_models = ['gpt-4.1-2025-04-14', 'Qwen2.5-14B-Instruct']\n",
    "# l_cols = [1, 1]\n",
    "l_pos = [\"top right\", \"bottom right\"]\n",
    "l_model_names = [\"GPT 4.1\", \"Qwen2.5 14B\"]\n",
    "# l_zero_shot_models = ['gpt-4.1-2025-04-14', \"claude-sonnet-4-20250514\"]\n",
    "# l_cols = [1, 2]\n",
    "\n",
    "for zero_shot_model, col_idx, pos, model_name in zip(l_zero_shot_models, l_cols, l_pos, l_model_names):\n",
    "    zero_shot_perf = df_pgr[(df_pgr['model'] == zero_shot_model) & (df_pgr['encoding_scheme'] == 'zero shot')]['PGR_pct'].iloc[0]\n",
    "\n",
    "    annotation_str = f\"{model_name} direct answering perf.\"\n",
    "    fig.add_hline(y=zero_shot_perf, line_dash='dash', annotation_text=annotation_str, col=col_idx, annotation_position=pos)\n",
    "\n",
    "fig.show('png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6dcde-fa6a-4f12-8e58-d4d5d162f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_pgr,\n",
    "              x='encoding_scheme',\n",
    "              y='backtranslation_bleu_scores',\n",
    "              color='model',\n",
    "              color_discrete_map={**d_claude_gradient, **d_gpt_gradient, **d_qwen_gradient},\n",
    "              markers=True,\n",
    "              # facet_col=\"Model Family\"\n",
    ")\n",
    "\n",
    "# needed for sorting by big model adherent+correct\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        categoryorder=\"array\",\n",
    "        # categoryarray=s_encoding_scheme_order\n",
    "        categoryarray=[' '.join(s.split('_')) for s in l_fixed_encoding_ordering]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=dict(text=title, font=dict(size=22)),\n",
    "    template=\"plotly_white\",\n",
    "    height=500,\n",
    "    width=1800,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.08,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01,\n",
    "        font=dict(size=10)\n",
    "    ),\n",
    "    margin=dict(t=110, r=30, b=80, l=70),\n",
    "    font=dict(size=13),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title=\"Encoding scheme\")\n",
    "fig.update_yaxes(range=[0, 101], dtick=10, title=\"BLEU score\")\n",
    "\n",
    "fig.add_hrect(\n",
    "    y0=60, y1=100,\n",
    "    fillcolor=\"#D3D3D3\", opacity=0.2,\n",
    "    layer=\"below\", line_width=0,\n",
    "    annotation_text=\"Fluent decoding\",\n",
    "    annotation_font=dict(color=\"grey\")\n",
    ")\n",
    "\n",
    "# fig.update_layout(width=600, height=600, title=None)\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_layout(height=510)\n",
    "\n",
    "fig.show('png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05d719-8bee-47b9-bf2d-ec240be479ff",
   "metadata": {},
   "source": [
    "# More plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557fec6-ac52-467e-8e7d-afe4b4e5757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def make_encoding_scheme_bar_plot(\n",
    "    df,\n",
    "    y_col=\"generated_cot_is_correct\",\n",
    "    title=\"MATH 500 Accuracy\",\n",
    "    y_axis_title=\"Accuracy\",\n",
    "    x_col=\"encoding_scheme\",\n",
    "    model_col=\"model\",\n",
    "    d_mapping=None,\n",
    "    yaxis_dtick=0.1,            # 10% ticks read cleaner\n",
    "    show_text=False,            # optionally show % text on bars\n",
    "    model_order=None,           # lock legend/order of models\n",
    "    font_family=\"Inter, Arial\", # consistent, modern\n",
    "    sort_by_col=None,\n",
    "    sort_agg=\"max\",\n",
    "    sort_desc=True\n",
    "):\n",
    "    # --- 1) DEFAULT SECTION MAP ---\n",
    "    if d_mapping is None:\n",
    "        d_mapping = {\n",
    "            \"baseline\": [\n",
    "                \"zero_shot\",\n",
    "                \"identity\"\n",
    "            ],\n",
    "            # \"letter mutation\": [\n",
    "            \"\": [\n",
    "                \"reverse_letters_in_each_word\",\n",
    "                \"reverse_letters_in_each_word_no_math_expressions\",\n",
    "                \"reverse_letters_in_each_word_only_math_expressions\",\n",
    "                \"swap_even_odd_letters_in_each_word\",\n",
    "                \"reverse_fibonacci_indices_in_each_word\",\n",
    "                \"letter_to_word_with_dot\",\n",
    "                \"dot_between_chars\",\n",
    "                \"space_between_chars\",\n",
    "            ],\n",
    "            \"language deletion\": [\"remove_all_verbs\", \"remove_all_nouns\"],\n",
    "            \"language translation\": [\n",
    "                \"French\",\"Chinese\",\"Korean\",\"Russian\",\"Arabic\",\"Adyghe\",\n",
    "                \"Morse_code\",\"Python\",\"enterprise_Java\",\n",
    "            ],\n",
    "            \"algorithmic cipher\": [\n",
    "                \"rot13_cipher\",\"base64_cipher\",\"base64_2x_cipher\",\n",
    "                \"base64_3x_cipher\",\"caesar_cipher\",\"gzip_to_base64_encoded\",\n",
    "            ],\n",
    "            # \"Wording of reasoning\": [\n",
    "            #     \"paraphrase_naive\",\n",
    "            #     \"pirate_speak\",\n",
    "            #     \"yoda_speak\",\n",
    "            #     \"shakespearean_text\",\n",
    "            # ],\n",
    "            # \"Distractors\": [\n",
    "            #     \"insert_tweet\",\n",
    "            #     \"python_snippet_comment\",\n",
    "            #     \"croissant_news_article\",\n",
    "            #     # \"math_textbook_article\",\n",
    "            #     # \"five_emojis\",\n",
    "            # ],\n",
    "            # \"Language\": [\n",
    "            #     \"leet_speak\",\n",
    "            #     \"rot13_cipher\"\n",
    "            # ],\n",
    "            # \"Inf. content\": [\n",
    "            #     \"replace_math_content_with_black_box\"\n",
    "            # ]\n",
    "        }\n",
    "\n",
    "    df_plot = df.copy()\n",
    "\n",
    "     # ===================== NEW SORTING LOGIC =====================\n",
    "    # Decide which column drives sorting\n",
    "    metric_col = sort_by_col or y_col\n",
    "\n",
    "    # Pick aggregation\n",
    "    agg_map = {\"max\": \"max\", \"mean\": \"mean\", \"median\": \"median\"}\n",
    "    agg_fn = agg_map.get(sort_agg, \"max\")\n",
    "\n",
    "    # Aggregate per scheme across models on the sorting metric\n",
    "    if metric_col not in df_plot.columns:\n",
    "        raise ValueError(f\"sort_by_col '{metric_col}' not found in dataframe.\")\n",
    "\n",
    "    sort_metric = (\n",
    "        df_plot\n",
    "        .groupby(x_col, as_index=False)[metric_col]\n",
    "        .agg(agg_fn)\n",
    "        .rename(columns={metric_col: \"sort_value\"})\n",
    "    )\n",
    "    # =============================================================\n",
    "\n",
    "    # --- 2) ORDERING BY SECTION, THEN BY MAX(Y) DESC ---\n",
    "    full_category_order, section_spans = [], []\n",
    "    present_set = set(df_plot[x_col].unique())\n",
    "    cursor = 0\n",
    "\n",
    "    for section_name, schemes in d_mapping.items():\n",
    "        present = [s for s in schemes if s in present_set]\n",
    "        if not present:\n",
    "            continue\n",
    "\n",
    "        section_df = sort_metric[sort_metric[x_col].isin(present)].copy()\n",
    "        section_df = section_df.sort_values(\"sort_value\", ascending=not sort_desc)\n",
    "        ordered = section_df[x_col].tolist()\n",
    "        if not ordered:\n",
    "            continue\n",
    "\n",
    "        start = cursor\n",
    "        full_category_order.extend(ordered)\n",
    "        cursor = len(full_category_order)\n",
    "        end = cursor - 1\n",
    "        section_spans.append((start, end, section_name))\n",
    "\n",
    "    if not full_category_order:\n",
    "        full_category_order = list(df_plot[x_col].unique())\n",
    "        section_spans = [(0, len(full_category_order) - 1, \"All\")]\n",
    "\n",
    "    df_plot[x_col] = pd.Categorical(df_plot[x_col], categories=full_category_order, ordered=True)\n",
    "\n",
    "    # --- 1) PRESERVE MODEL ORDER FROM DATAFRAME ---\n",
    "    # Use the order models appear in df instead of sorting alphabetically\n",
    "    if model_order is None:\n",
    "        model_order = list(dict.fromkeys(df_plot[model_col]))  # <-- preserves original order\n",
    "    df_plot[model_col] = pd.Categorical(df_plot[model_col], categories=model_order, ordered=True)\n",
    "\n",
    "    # --- 3) ERROR BARS (auto-detect) ---\n",
    "    err_hi_col = f\"{y_col}_hi_ci\"\n",
    "    err_lo_col = f\"{y_col}_low_ci\"\n",
    "    error_y = err_hi_col if err_hi_col in df_plot.columns else None\n",
    "    error_y_minus = err_lo_col if err_lo_col in df_plot.columns else None\n",
    "\n",
    "    def wrap_string(s: str, width: int = 15) -> str:\n",
    "        \"\"\"\n",
    "        Wraps the input string so that each line has at most `width` characters.\n",
    "        If a word exceeds the width, a '-' and newline are inserted.\n",
    "    \n",
    "        Args:\n",
    "            s (str): Input string to wrap.\n",
    "            width (int): Maximum number of characters per line. Default is 15.\n",
    "    \n",
    "        Returns:\n",
    "            str: The wrapped string.\n",
    "        \"\"\"\n",
    "        result = \"\"\n",
    "        i = 0\n",
    "    \n",
    "        while i < len(s):\n",
    "            # Take a chunk of 'width' characters\n",
    "            chunk = s[i:i+width]\n",
    "            # If the chunk is exactly width long and not at the end, add a dash\n",
    "            if len(chunk) == width and i + width < len(s):\n",
    "                result += chunk + \"<br>\"\n",
    "            else:\n",
    "                result += chunk\n",
    "            i += width\n",
    "    \n",
    "        return result\n",
    "\n",
    "    # Label prettifier\n",
    "    def prettify(s: str) -> str:\n",
    "        s = s.replace(\"_\", \"<br>\")\n",
    "        # return wrap_string(s, 10)\n",
    "        return s\n",
    "\n",
    "    # --- 2) DETECT WHETHER Y-VALUES ARE PERCENTAGES OR NUMERICAL ---\n",
    "    y_min, y_max = df_plot[y_col].min(), df_plot[y_col].max()\n",
    "    is_percent = y_max <= 1.0 and y_min >= 0.0\n",
    "\n",
    "    # Set default dtick depending on scale\n",
    "    if yaxis_dtick is None:\n",
    "        yaxis_dtick = 0.1 if is_percent else None\n",
    "    \n",
    "    # --- 2) BASE FIGURE (no change here, just ensures model order applies)\n",
    "    fig = px.bar(\n",
    "        df_plot,\n",
    "        x=x_col,\n",
    "        y=y_col,\n",
    "        color=model_col,\n",
    "        category_orders={x_col: full_category_order, model_col: model_order},\n",
    "        barmode=\"group\",\n",
    "        template=\"plotly_white\",\n",
    "        # color_discrete_sequence=px.colors.qualitative.Safe,\n",
    "        color_discrete_map={**d_qwen_gradient, **d_gpt_gradient},\n",
    "        title=title,\n",
    "        height=600,\n",
    "        width=1800,\n",
    "        error_y=error_y,\n",
    "        error_y_minus=error_y_minus,\n",
    "        text=(df_plot[y_col] if show_text else None),\n",
    "    )\n",
    "\n",
    "    # Y axis as percent\n",
    "    if is_percent:\n",
    "        fig.update_yaxes(\n",
    "            title=y_axis_title,\n",
    "            tickformat=\".0%\",\n",
    "            tickmode=\"linear\",\n",
    "            dtick=yaxis_dtick,\n",
    "            rangemode=\"tozero\"\n",
    "        )\n",
    "    else:\n",
    "        fig.update_yaxes(\n",
    "            title=y_axis_title,\n",
    "            tickmode=\"linear\",\n",
    "            rangemode=\"tozero\",\n",
    "            dtick=yaxis_dtick\n",
    "        )\n",
    "\n",
    "    # X axis: rely on categoryarray for order; set readable tick labels\n",
    "    fig.update_xaxes(\n",
    "        title=\"Encoding scheme\",\n",
    "        ticktext=[f\"<b>{prettify(lbl)}</b>\" for lbl in full_category_order],\n",
    "        tickvals=full_category_order,       # use category values, not numeric indices\n",
    "        tickangle=0\n",
    "    )\n",
    "\n",
    "    # Bar spacing and text formatting\n",
    "    fig.update_traces(\n",
    "        texttemplate = \"%{y:.0%}\" if (is_percent and show_text) else \"%{y:}\" if show_text else None,\n",
    "        textposition=\"outside\" if show_text else \"none\",\n",
    "        # Pass extra fields if you want them in hover (optional)\n",
    "        customdata=df_plot[[model_col]].values,\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        bargap=0.15,\n",
    "        bargroupgap=0.05,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.08,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01,\n",
    "            font=dict(size=11)  # ↓ Smaller legend font\n",
    "        ),\n",
    "        margin=dict(t=110, r=30, b=80, l=70),\n",
    "        font=dict(family=font_family, size=15),\n",
    "        title=dict(font=dict(size=22))\n",
    "    )\n",
    "    \n",
    "    # Make x-axis tick labels smaller\n",
    "    fig.update_xaxes(\n",
    "        ticktext=[f\"<b>{prettify(lbl)}</b>\" for lbl in full_category_order],\n",
    "        tickvals=full_category_order,\n",
    "        tickangle=0,\n",
    "        tickfont=dict(size=11)  # ↓ Smaller x-axis font size\n",
    "    )\n",
    "\n",
    "    # --- 5) SECTION BACKGROUNDS + LABELS ---\n",
    "    N = len(full_category_order)\n",
    "\n",
    "    # Helper: convert [start,end] (category indices) to x domain [0,1]\n",
    "    def to_xdomain(idx):\n",
    "        # position of left edge of category idx:\n",
    "        return idx / N\n",
    "\n",
    "    # alternating light bands for sections\n",
    "    shapes = []\n",
    "    annotations = []\n",
    "    for i, (start_idx, end_idx, name) in enumerate(section_spans):\n",
    "        x0 = to_xdomain(start_idx)\n",
    "        x1 = to_xdomain(end_idx + 1)\n",
    "        band = dict(\n",
    "            type=\"rect\",\n",
    "            xref=\"x domain\", yref=\"paper\",\n",
    "            x0=x0, x1=x1, y0=0, y1=1,\n",
    "            layer=\"below\",\n",
    "            line=dict(width=0),\n",
    "            fillcolor=\"rgba(0,0,0,0.03)\" if i % 2 == 0 else \"rgba(0,0,0,0.00)\"\n",
    "        )\n",
    "        shapes.append(band)\n",
    "        annotations.append(dict(\n",
    "            x=(x0 + x1) / 2, xref=\"x domain\",\n",
    "            y=1.06, yref=\"paper\",\n",
    "            text=name, showarrow=False,\n",
    "            font=dict(size=13, color=\"black\"),\n",
    "            xanchor=\"center\"\n",
    "        ))\n",
    "    fig.update_layout(shapes=shapes)\n",
    "    for a in annotations:\n",
    "        fig.add_annotation(**a)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72f4a6-5246-4397-890a-3eb3ded732a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_encoding_scheme_bar_plot(\n",
    "    # df=df_pgr,\n",
    "    df=df_viz_tmp,#[df_viz_tmp['encoding_scheme'] == 'letter_to_word_with_dot'],\n",
    "    y_col=\"adherent_and_correct\",\n",
    "    # y_col=\"PGR_pct\",\n",
    "    title=\"MATH 500 Accuracy\",\n",
    "    y_axis_title=\"Accuracy\",\n",
    "    yaxis_dtick=0.1,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    ")\n",
    "# fig.update_yaxes(range=[0, 1])\n",
    "\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a704022-7b09-456f-8dc3-81ce9ae100c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_viz_tmp[df_viz_tmp['model'] == 'gpt-4.1-2025-04-14'],\n",
    "    y_col=\"adherent_and_correct\",\n",
    "    # title=\"Proportion of encoding adherent & correct responses on MATH 500\",\n",
    "    title=None,\n",
    "    y_axis_title=\"% of responses adherent & correct\",\n",
    "    yaxis_dtick=0.05,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    "\n",
    ")\n",
    "\n",
    "fig.update_yaxes(range=[0, 0.21], dtick=0.03)\n",
    "fig.update_layout(width=800, height=600)\n",
    "\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b86a2e-c97f-4ee2-ad57-1aef0c0e4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute PGR (percent of identity performance) ---\n",
    "df_pgr = df_viz_tmp.copy()\n",
    "\n",
    "# df_pgr = df_pgr[~( (df_pgr['input_type'] == 'mathcot_fewshot') & (df_pgr['encoding_scheme'] == 'identity') )]\n",
    "\n",
    "\n",
    "df_pgr['model'] = df_pgr['model'].str.split('Qwen/').str[-1]\n",
    "\n",
    "# df_pgr = df_pgr[~df_pgr['model'].str.contains('gpt')]\n",
    "# df_pgr = df_pgr[df_pgr['model'].str.contains('14B')]\n",
    "\n",
    "# Identity baseline per model\n",
    "identity_baseline = (\n",
    "    df_pgr[df_pgr[\"encoding_scheme\"] == \"identity\"]\n",
    "    .set_index(\"model\")[\"adherent_and_correct\"]\n",
    ")\n",
    "\n",
    "df_pgr[\"identity_value\"] = np.maximum(df_pgr[\"model\"].map(identity_baseline), 0.0001)\n",
    "\n",
    "# Avoid divide-by-zero\n",
    "# df_pgr = df_pgr[df_pgr[\"identity_value\"] > 0].copy()\n",
    "\n",
    "# Mean PGR as percentage\n",
    "df_pgr[\"PGR_pct\"] = (df_pgr[\"adherent_and_correct\"] / df_pgr[\"identity_value\"])\n",
    "\n",
    "# CI deltas -> percentage deltas relative to identity baseline\n",
    "# low is negative, high is positive\n",
    "df_pgr[\"PGR_pct_hi_ci\"]  = (df_pgr[\"adherent_and_correct_hi_ci\"] / df_pgr[\"identity_value\"])\n",
    "df_pgr[\"PGR_pct_low_ci\"] = (df_pgr[\"adherent_and_correct_low_ci\"]        / df_pgr[\"identity_value\"])\n",
    "\n",
    "df_pgr.loc[df_pgr[\"encoding_scheme\"] == \"identity\", [\"PGR_pct_hi_ci\", \"PGR_pct_low_ci\"]] = 0.0\n",
    "\n",
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_pgr,\n",
    "    # df=df_pgr[df_pgr['model'] == 'gpt-4.1-2025-04-14'],\n",
    "    y_col=\"PGR_pct\",\n",
    "    # title=\"Relative % of responses adherent & correct vs. identity encoding\",\n",
    "    title=None,\n",
    "    y_axis_title=\"% of identity adherent & correct\",\n",
    "    yaxis_dtick=0.1,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    ")\n",
    "\n",
    "# fig.update_yaxes(range=[0.0, 1.05])\n",
    "# fig.update_yaxes(range=[0.0, 0.35], dtick=0.05)\n",
    "# fig.update_layout(height=475)\n",
    "# fig.update_traces(width=0.5)\n",
    "# fig.update_layout(width=800, height=600)\n",
    "\n",
    "\n",
    "fig.show('png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb65175-68f3-4405-b842-e8bf48e3e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_viz_tmp,\n",
    "    y_col=\"cot_gt_logprobs\",\n",
    "     title=\"Log loss on MATH 500 ground-truth encoded CoT from PRM800K\",\n",
    "    y_axis_title=\"Mean log loss per sequence\",\n",
    "    yaxis_dtick=250,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644330e7-ca98-47a0-8883-6e2d35183649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "\n",
    "df_viz_tmp_plot = df_viz_tmp_plot[df_viz_tmp_plot['encoding_scheme'] != 'zero_shot']\n",
    "\n",
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_viz_tmp_plot,\n",
    "    y_col=\"backtranslation_bleu_scores\",\n",
    "     title=\"Encoded -> English translation BLEU\",\n",
    "    y_axis_title=\"Encoded -> English translation BLEU\",\n",
    "    yaxis_dtick=10,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    "\n",
    ")\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f46b8d-efe4-47b3-ba32-51c6a5e28575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "\n",
    "df_viz_tmp_plot = df_viz_tmp_plot[df_viz_tmp_plot['encoding_scheme'] != 'zero_shot']\n",
    "\n",
    "fig = make_encoding_scheme_bar_plot(\n",
    "    df=df_viz_tmp_plot,\n",
    "    y_col=\"total_translation_loss\",\n",
    "     title=\"Encoded -> English translation log loss\",\n",
    "    y_axis_title=\"Mean log loss per sequence\",\n",
    "    yaxis_dtick=50,\n",
    "    sort_by_col=\"adherent_and_correct\"\n",
    "\n",
    ")\n",
    "fig.show('png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a837441-3b1d-4760-9fa4-6575058468c0",
   "metadata": {},
   "source": [
    "# Plot the translation ability -> acc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6dbd1-8f91-4387-abb2-e162bbd99345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def styled_logistic_scatter_faceted(\n",
    "    df,\n",
    "    facet_col=\"model\",\n",
    "    x_col=\"backtranslation_gt_logprobs\",\n",
    "    y_col=\"adherent_and_correct\",\n",
    "    title=\"Adherence vs. Backtranslation Log-Prob\",\n",
    "    x_axis_title=\"Backtranslation log-prob\",\n",
    "    y_axis_title=\"Adherent & correct\",\n",
    "    font_family=\"Inter, Arial\",\n",
    "    marker_size=7,\n",
    "    opacity=0.6,\n",
    "    legend_font_size=10,\n",
    "    x_tick_font_size=10,\n",
    "    line_width=3,\n",
    "    r2_digits=3,               # <— precision for R² display\n",
    "    keep_subplot_title=True,\n",
    "    max_pow_override=None\n",
    "):\n",
    "    def logistic(x, L, k, x0):\n",
    "        return L / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "    # overall y-scale to decide percent formatting\n",
    "    y_all = df[y_col].to_numpy(dtype=float)\n",
    "    is_percent = (np.nanmin(y_all) >= 0) and (np.nanmax(y_all) <= 1.0)\n",
    "\n",
    "    # facet ordering\n",
    "    if pd.api.types.is_categorical_dtype(df[facet_col]):\n",
    "        facet_values = [c for c in df[facet_col].cat.categories if (df[df[facet_col]==c].shape[0] > 0)]\n",
    "    else:\n",
    "        d_model_to_size = dict(zip(df['model'], df['model_size']))\n",
    "        facet_values = sorted(df[facet_col].dropna().unique(), key=lambda x: d_model_to_size.get(x, x))\n",
    "\n",
    "    print(facet_values)\n",
    "\n",
    "    n_cols = max(1, len(facet_values))\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=n_cols, shared_yaxes=True, horizontal_spacing=0.06,\n",
    "        subplot_titles=[f\"{facet_col} = {v}\" if keep_subplot_title else None for v in facet_values],\n",
    "    )\n",
    "\n",
    "    x_positive = df[df[x_col] > 0][x_col]\n",
    "    min_x, max_x = x_positive.min(), x_positive.max()\n",
    "    # Create tick values at decade intervals\n",
    "    min_pow = int(np.floor(np.log2(min_x)))\n",
    "    max_pow = int(np.ceil(np.log2(max_x)))\n",
    "    tick_vals = [2 ** p for p in range(min_pow, max_pow + 1)]\n",
    "    if max_pow_override:\n",
    "        if tick_vals[-1] > max_pow_override:\n",
    "            tick_vals[-1] = max_pow_override\n",
    "    tick_text = [f\"{v:g}\" for v in tick_vals]\n",
    "\n",
    "    for i, val in enumerate(facet_values, start=1):\n",
    "        sub = df[df[facet_col] == val]\n",
    "        x = sub[x_col].to_numpy(dtype=float)\n",
    "        y = sub[y_col].to_numpy(dtype=float)\n",
    "\n",
    "        # points\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x, y=y, mode=\"markers+text\", name=f\"Points ({val})\",\n",
    "                marker=dict(size=marker_size, line=dict(width=0)),\n",
    "                opacity=opacity, showlegend=False,\n",
    "                # text=sub[\"encoding_scheme\"] + \"-\" + sub[\"model\"],\n",
    "                hovertemplate=\"<b>%{text}</b><extra></extra>\",  # <-- Just encoding scheme\n",
    "            ),\n",
    "            row=1, col=i\n",
    "        )\n",
    "\n",
    "        # choose regressor space\n",
    "        # linear_on_log_x = True\n",
    "        # if linear_on_log_x:\n",
    "        #     valid = (x > 0) & np.isfinite(x) & np.isfinite(y)\n",
    "        #     X = np.log2(x[valid])\n",
    "        #     Y = y[valid]\n",
    "        # else:\n",
    "        #     valid = np.isfinite(x) & np.isfinite(y)\n",
    "        #     X = x[valid]\n",
    "        #     Y = y[valid]\n",
    "\n",
    "        # if X.size >= 2 and np.nanstd(Y) > 0:\n",
    "        #     b1, b0 = np.polyfit(X, Y, 1)   # Y = b1*X + b0\n",
    "        #     # make a smooth line across current x-range\n",
    "        #     x_line = np.linspace(np.nanmin(x[valid]), np.nanmax(x[valid]), 300)\n",
    "        #     X_line = np.log2(x_line) if linear_on_log_x else x_line\n",
    "        #     y_line = b1 * X_line + b0\n",
    "\n",
    "        #     # R^2 on observed points (in same space used to fit)\n",
    "        #     y_hat = b1 * X + b0\n",
    "        #     ss_res = np.nansum((Y - y_hat) ** 2)\n",
    "        #     ss_tot = np.nansum((Y - np.nanmean(Y)) ** 2)\n",
    "        #     if ss_tot > 0 and np.isfinite(ss_res):\n",
    "        #         r2 = 1 - ss_res / ss_tot\n",
    "        #         r2_text = f\"{r2:.{r2_digits}f}\"\n",
    "\n",
    "        #     fig.add_trace(\n",
    "        #         go.Scatter(\n",
    "        #             x=x_line, y=y_line, mode=\"lines\",\n",
    "        #             name=f\"Linear fit ({val})\",\n",
    "        #             # line=linear_line_kwargs,\n",
    "        #             showlegend=(i == 1),\n",
    "        #         ),\n",
    "        #         row=1, col=i\n",
    "        #     )\n",
    "        \n",
    "        # logistic fit + R^2\n",
    "        r2_text = \"—\"\n",
    "        try:\n",
    "            if np.isfinite(x).sum() >= 3 and np.nanstd(y) > 0:\n",
    "                p0 = [np.nanmax(y), 1.0, np.nanmedian(x)]\n",
    "                params, _ = curve_fit(logistic, x, y, p0=p0, maxfev=10000)\n",
    "                L, k, x0 = params\n",
    "\n",
    "                x_fit = np.linspace(np.nanmin(x), max(np.nanmax(x), 98), 300)\n",
    "                y_fit = logistic(x_fit, L, k, x0)\n",
    "\n",
    "                # R^2 on observed x\n",
    "                y_hat = logistic(x, L, k, x0)\n",
    "                ss_res = np.nansum((y - y_hat) ** 2)\n",
    "                ss_tot = np.nansum((y - np.nanmean(y)) ** 2)\n",
    "                if ss_tot > 0 and np.isfinite(ss_res):\n",
    "                    r2 = 1 - ss_res / ss_tot\n",
    "                    r2_text = f\"{r2:.{r2_digits}f}\"\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=x_fit, y=y_fit, mode=\"lines\",\n",
    "                        name=f\"Logistic fit ({val})\",\n",
    "                        line=dict(width=1, color=\"black\", dash=\"dash\"),\n",
    "                        showlegend=(i == 1),\n",
    "                    ),\n",
    "                    row=1, col=i\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        # R^2 annotation (top-left of this subplot)\n",
    "        fig.add_annotation(\n",
    "            x=0.1, y=0.55,\n",
    "            xref=f\"x{i}\" if i > 1 else \"x\",   # ✅ Use proper axis names\n",
    "            yref=f\"y{i}\" if i > 1 else \"y\",\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"top\",\n",
    "            text=f\"R² = {r2_text}\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=12),\n",
    "            align=\"left\",\n",
    "            bgcolor=\"rgba(255,255,255,0.6)\",\n",
    "            bordercolor=\"rgba(0,0,0,0.2)\",\n",
    "            borderwidth=1,\n",
    "        )\n",
    "\n",
    "        # axes cosmetics\n",
    "        fig.update_xaxes(\n",
    "            title=x_axis_title if i == 1 else \"\",\n",
    "            tickfont=dict(size=x_tick_font_size),\n",
    "            zeroline=False,\n",
    "            type=\"log\",\n",
    "            # autorange=\"reversed\",\n",
    "            tickvals=tick_vals,\n",
    "            ticktext=tick_text,\n",
    "            row=1, col=i,\n",
    "            dtick=0.1\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            tickformat=\".0%\" if is_percent else \",\",\n",
    "            rangemode=\"tozero\",\n",
    "            zeroline=False,\n",
    "            dtick=0.05,\n",
    "            row=1, col=i,\n",
    "            title=y_axis_title\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title=dict(text=title, font=dict(size=18)),\n",
    "        height=600,\n",
    "        width=350 * n_cols if n_cols <= 3 else 300 * n_cols,\n",
    "        font=dict(family=font_family, size=14),\n",
    "        margin=dict(t=0, r=0, b=0, l=0),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.08, xanchor=\"left\", x=0,\n",
    "                    font=dict(size=legend_font_size)),\n",
    "        showlegend=False\n",
    "    )\n",
    "    # global axis labels\n",
    "    # fig.add_annotation(text=x_axis_title, showarrow=False, xref=\"paper\", yref=\"paper\",\n",
    "    #                    x=0.5, y=-0.18, font=dict(size=14))\n",
    "    # fig.add_annotation(text=y_axis_title, showarrow=False, xref=\"paper\", yref=\"paper\",\n",
    "    #                    x=-0.06, y=0.5, textangle=-90, font=dict(size=14))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ---- Use it on your dataframe ----\n",
    "df_viz_tmp_plot = df_viz_tmp.copy()\n",
    "\n",
    "start_len = len(df_viz_tmp_plot)\n",
    "df_viz_tmp_plot = df_viz_tmp_plot[~df_viz_tmp_plot['encoding_scheme'].isin(['identity', 'zero_shot'])]\n",
    "assert len(df_viz_tmp_plot) != start_len\n",
    "\n",
    "df_viz_tmp_plot[\"placeholder_col\"] = \"1\"\n",
    "\n",
    "d_zero_shot_baseline =  (\n",
    "    df_pgr[df_pgr[\"encoding_scheme\"] == \"zero_shot\"]\n",
    "    .set_index(\"model\")[\"adherent_and_correct\"]\n",
    ")\n",
    "\n",
    "fig = styled_logistic_scatter_faceted(\n",
    "    # df_viz_tmp_plot[df_viz_tmp_plot['backtranslation_bleu_scores'].notna() & ~df_viz_tmp_plot['model'].str.contains('gpt')],\n",
    "    #[df_viz_tmp_plot['adherent_and_correct'] > 0.01],\n",
    "    df_viz_tmp_plot,\n",
    "    # df_viz_tmp_plot[(df_viz_tmp_plot['adherent_and_correct'] >= df_viz_tmp_plot['model'].map(d_zero_shot_baseline))],\n",
    "    # x_col=\"backtranslation_gt_logprobs\",\n",
    "    x_col=\"backtranslation_bleu_scores\",\n",
    "    y_col=\"adherent_and_correct\",\n",
    "    # y_col='cot_gt_logprobs',\n",
    "    # title=\"% of responses adherent & correct vs. encoded -> English translation BLEU\",\n",
    "    title=None,\n",
    "    x_axis_title=\"BLEU score, decoding encoded text to English\",\n",
    "    # x_axis_title=\"Encoded text to English text log loss\",\n",
    "    y_axis_title=\"% of MATH500 responses adherent & correct\",\n",
    "    # facet_col=\"model\",\n",
    "    facet_col=\"placeholder_col\",\n",
    "    keep_subplot_title=False,\n",
    "    max_pow_override=100,\n",
    "    x_tick_font_size=12\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(width=600, height=500, title=None)\n",
    "# fig.update_layout(width=1600, height=1200, title=None)\n",
    "# --- shaded region instead of vline ---\n",
    "x_cut = 60\n",
    "x_right = float(df_viz_tmp_plot[\"backtranslation_bleu_scores\"].max()) + 30\n",
    "# x_cut = 0.2\n",
    "# x_right = 0.02\n",
    "\n",
    "\n",
    "fig.add_vrect(\n",
    "    x0=x_cut, x1=x_right,           # shade from cutoff to the right edge of data\n",
    "    fillcolor=\"#D3D3D3\", opacity=0.25,\n",
    "    line_width=0,\n",
    "    layer=\"below\",\n",
    "    row=1, col=1                    # adjust if you facet into multiple columns\n",
    ")\n",
    "\n",
    "# label for the shaded region\n",
    "fig.add_annotation(\n",
    "    x=0.84,\n",
    "    # x=0.85,\n",
    "    xref=\"paper\",           # data coordinates\n",
    "    y=0.99, yref=\"paper\",\n",
    "    text=\"fluent translation\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"top\",\n",
    "    bgcolor=\"rgba(255,255,255,0.0)\",\n",
    "    bordercolor=\"rgba(0,0,0,0.0)\",\n",
    "    borderwidth=0,\n",
    "    font=dict(size=9)\n",
    ")\n",
    "fig.update_traces(textposition=\"bottom left\")\n",
    "\n",
    "\n",
    "fig.show('png')\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e15b74-d9a1-4718-bdbd-83b80f0b453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test_hashes = ['f01404b75a072276ac1ca6f131313a8a615dbb99', '481e3a2a1f521438c8938721b0df08a389e3406f', 'b411a66e1c2f9ca7a79c065099879f202990c9a2', '33c2595336eb7c74c6820302498fd097fa52463c', 'e3f76d94751a09af21005696ac072219143ebc58', 'b77d4d2334cc5dad12f2168d5fa48749622f34af', '2007b98cc05baeb988564f2e51d6f496a8afe7b8', 'f0a6d8673396c2f4415e10c2fc13effa262fe50d', '16b151e08633744e1c501d6f41707177c5aa6125', 'a846c2161731fabfe9b9d371ac05473c5ba9289d', 'a9a752f3c050a16ef74c6ad9436f0aa6291fb1ff', 'b4c85c27bcc89f916f50867f974bd13875416067']\n",
    "\n",
    "for hash in l_test_hashes:\n",
    "    target_path = f\"/home/ubuntu/sky_workdir/encoding-schemes/output/{hash}/data/sft_train.parquet\"\n",
    "    if os.path.exists(target_path):\n",
    "        print(duckdb.query(f\"SELECT SUM(num_tokens) FROM read_parquet('{target_path}')\").to_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd132a-4f07-4354-8aef-ecb744066d02",
   "metadata": {},
   "source": [
    "# Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcaf2b4-16fb-491c-966f-85ea99452d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoding_schemes import get_deterministic_adherence_fn\n",
    "from encoding_schemes.letter_permutations import get_English_dictionary, reverse_letters_in_each_word, normalize_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea91e7cf-1be3-48f0-be20-ce1918c231b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scheme = 'speaking_letter_to_word_with_dot'\n",
    "test_model = 'Qwen/Qwen2.5-14B-Instruct'\n",
    "test_idx = 21\n",
    "\n",
    "adherence_fn = get_deterministic_adherence_fn(test_scheme, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23894a-6e69-4aa1-af84-dbdab60c1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['reference_problem_df'].iloc[0][test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cecb8de-42aa-4717-9646-64d7528f6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['reference_solution_df'].iloc[0][test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4feb0b5-0009-474e-b920-63453ce01ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['generated_cots_df'].iloc[0][test_idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9a283-2629-4ac5-ba34-b50b90764adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_letters_in_each_word(df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['generated_cots_df'].iloc[0][test_idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f92dd3-24db-4bd9-94cf-c25342c3d6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['generated_cot_adhered_encoding_style'].iloc[0]#[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561fed07-320b-47cf-bedd-c2cf7d2af6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_hash = df_viz[(df_viz['model'] == test_model) & (df_viz['encoding_scheme'] == test_scheme)]['experiment_hash'].iloc[0]\n",
    "example_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd34c8-c820-4689-9865-b5287e3fa4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/home/ubuntu/sky_workdir/encoding-schemes/output/{example_hash}/data/sft_model_meta.json\", \"r\") as fp:\n",
    "    d_example = json.load(fp)\n",
    "\n",
    "d_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4a42f-b0d6-4c25-a09c-45c80a4e2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_test_path = f\"/home/ubuntu/sky_workdir/encoding-schemes/output/{example_hash}/data/sft_train.parquet\"\n",
    "\n",
    "df_sft = pd.read_parquet(sft_test_path)\n",
    "df_sft['messages'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852ab00-13eb-4aec-88f0-5475f3cb3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23601359-7ca9-4014-a9ef-2622cd58edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft = df_sft.iloc[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ecbbd-d047-4a5c-9923-fa5754e889ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft['num_tokens'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49010c-5082-487b-b74c-d76d9b8d0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sft_parquet_to_jsonl(df, output_json_path):\n",
    "    n_rows_written = 0\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, raw in enumerate(df[\"messages\"]):\n",
    "\n",
    "            json_line = {\"messages\": list(raw)}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + \"\\n\")\n",
    "            n_rows_written += 1\n",
    "\n",
    "    print(f\"[prep] Wrote {n_rows_written} training rows to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e777b-73e2-409d-9443-837fad99fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_sft_parquet_to_jsonl(df_sft, sft_test_path.replace(\"parquet\", \"jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c2b6a-dbdc-4567-a523-0c537a456126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/output/e4a87a8626efeeb6df76d94bb34e7e5e34c77154/data/sft_train.parquet\")\n",
    "\n",
    "df_sft = df_sft.iloc[:9120]\n",
    "\n",
    "df_sft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2057b7-3657-40c5-ab01-9fa47155aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft['num_tokens'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33d894-e729-4149-95d6-2e3eeb8c0dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
