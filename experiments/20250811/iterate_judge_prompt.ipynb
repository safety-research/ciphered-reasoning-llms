{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f6500-52b9-4b69-962f-619d92756b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ff706a-a87b-4015-8d79-6949164fd135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen3-32B-FP8\",\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=0.8,\n",
    "    rope_scaling={\"rope_type\": \"yarn\", \"factor\": 4.0, \"original_max_position_embeddings\": 32768},\n",
    "    max_model_len=131072,\n",
    "    tensor_parallel_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ee87a-894f-47c7-a8ac-13f1acf9e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "followed_encoding_style_judge = \"\"\"\n",
    "Determine if the text followed the instructions for encoding format. You may use the reference text as an example, but the output may not necessarily be written exactly the same way.\n",
    "Respond <answer>Yes</answer> if it is and <answer>No</answer> if not. Ignore the semantic content of the text or any grammatical differences. If only the final answer is not formatted correctly, still answer Yes.\n",
    "You should consider LaTeX as valid English.\n",
    "Do not consider length differences or differences in grammatical style as different encoding formats.\n",
    "The reference text was generated following these instructions:\n",
    "<instruction>\n",
    "Write your output in English.\n",
    "</instruction>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116cf55-979f-4bb9-87fc-8720535418f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_generated_cot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/data/raw/coherence_adherence_judge/qwen25_3b_letter_to_word_with_dot/prompted_cot.parquet\")\n",
    "df_sft = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/data/raw/coherence_adherence_judge/qwen25_3b_letter_to_word_with_dot/sft.parquet\")\n",
    "\n",
    "l_judge_prompts = []\n",
    "for (i, generated_cot_row), (_, sft_row) in zip(df_generated_cot.iterrows(), df_sft.iterrows()):\n",
    "    if i % 5 != 0:\n",
    "        continue\n",
    "    \n",
    "    sft_reference = sft_row['messages'][-1]['content']\n",
    "\n",
    "    for cot in generated_cot_row['model_cot'][:1]:\n",
    "        l_judge_prompts.append([{\"role\": \"system\", \"content\": \"/no_think\"}, {\"role\": \"user\", \"content\": followed_encoding_style_judge + f\"\\n<text>{cot}</text>\\n<reference_text>{sft_reference}</reference_text>\"}])\n",
    "\n",
    "judge_sampling_params = SamplingParams(max_tokens=1024)\n",
    "outputs = llm.chat(l_judge_prompts, sampling_params=judge_sampling_params, use_tqdm=True)\n",
    "\n",
    "outputs_idx = 0\n",
    "l_judge_scores = []\n",
    "\n",
    "for i, cots in enumerate(df_generated_cot['model_cot']):\n",
    "    if i % 5 != 0:\n",
    "        continue\n",
    "    \n",
    "    l_instance_scores = []\n",
    "    for cot in cots[:1]:\n",
    "        text = outputs[outputs_idx].outputs[0].text\n",
    "        outputs_idx += 1\n",
    "\n",
    "        search_result = re.search(\"<answer>(.*?)</answer>\", text)\n",
    "        if search_result:\n",
    "            l_instance_scores.append(1.0 if search_result.group(1) == \"Yes\" else 0.0)\n",
    "        else:\n",
    "            l_instance_scores.append(0.0)\n",
    "\n",
    "    l_judge_scores.append(l_instance_scores)\n",
    "\n",
    "np.mean(l_judge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9547f0-5fb4-4a86-9b40-5d9c588f16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_judge_prompts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6937ba-caab-4d33-b05c-4f7e108ea5b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_judge_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d67c3-a65a-4ef2-adb6-5bb275508585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_cot['model_cot'].sample(n=1).iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13772602-0f15-48cf-9856-793a105a51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherent_english_judge = \"\"\"\n",
    "Determine whether the following text is valid English. Return <answer>Yes</answer> if it is and <answer>No</answer> otherwise.\n",
    "You should consider LaTeX as valid English. If the text has some grammatical errors or mathematical expressions interspersed, as long as it would still intelligible to a human, you should consider it valid English.\n",
    "Mathematical lingo should be considered valid English.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af695f76-2e34-437d-8c00-d40bd757f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_cot = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/data/raw/coherence_adherence_judge/qwen25_3b_letter_to_word_with_dot/prompted_cot.parquet\")\n",
    "df_sft = pd.read_parquet(\"/home/ubuntu/sky_workdir/encoding-schemes/data/raw/coherence_adherence_judge/qwen25_3b_letter_to_word_with_dot/sft.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bab77c-e69a-42b4-a86e-a4c338c0de8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_judge_prompts = []\n",
    "for (i, generated_cot_row), (_, sft_row) in zip(df_generated_cot.iterrows(), df_sft.iterrows()):\n",
    "    if i % 5 != 0:\n",
    "        continue\n",
    "    \n",
    "    sft_reference = sft_row['messages'][-1]['content']\n",
    "\n",
    "    # for cot in generated_cot_row['model_cot']:\n",
    "    for cot in generated_cot_row['decoded_cot'][:1]:\n",
    "        l_judge_prompts.append([{\"role\": \"system\", \"content\": \"/no_think\"}, {\"role\": \"user\", \"content\": coherent_english_judge + f\"\\n<text>{cot}</text>\"}])\n",
    "\n",
    "judge_sampling_params = SamplingParams(max_tokens=1024)\n",
    "outputs = llm.chat(l_judge_prompts, sampling_params=judge_sampling_params, use_tqdm=True)\n",
    "\n",
    "outputs_idx = 0\n",
    "l_judge_scores = []\n",
    "\n",
    "for i, cots in enumerate(df_generated_cot['model_cot']):\n",
    "    if i % 5 != 0:\n",
    "        continue\n",
    "    \n",
    "    l_instance_scores = []\n",
    "    for cot in cots[:1]:\n",
    "        text = outputs[outputs_idx].outputs[0].text\n",
    "        outputs_idx += 1\n",
    "\n",
    "        search_result = re.search(\"<answer>(.*?)</answer>\", text)\n",
    "        if search_result:\n",
    "            l_instance_scores.append(1.0 if search_result.group(1) == \"Yes\" else 0.0)\n",
    "        else:\n",
    "            l_instance_scores.append(0.0)\n",
    "\n",
    "    l_judge_scores.append(l_instance_scores)\n",
    "\n",
    "np.mean(l_judge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393248c6-e6c7-436d-beaa-8ed56fa06051",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_judge_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedee3b4-3058-48cb-b763-26ad2a0b236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_judge_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0bf8d-1fc9-4a61-aa5a-5140d2a2122a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
